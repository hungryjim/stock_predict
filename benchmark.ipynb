{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "benchmark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhagNmQAJESp",
        "colab_type": "code",
        "outputId": "2295668a-52ee-43a7-ca3d-03edea95bc77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# get all the imports\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from keras.models import load_model\n",
        "from prepare_data_for_modal import prepare_data, series_to_supervised, split_sequence\n",
        "\n",
        "\n",
        "pre_stock_data, stock, news = prepare_data('1')\n",
        "\n",
        "poly=PolynomialFeatures(degree=3)\n",
        "stock=poly.fit_transform(stock)\n",
        "\n",
        "# Split the data into train and test data¶\n",
        "train_x = np.hstack([stock[:400], news[:400]])\n",
        "# y就是要被预测的价格\n",
        "train_y = pre_stock_data.iloc[:406, 9:].values\n",
        "train_y = series_to_supervised(train_y, 6, 1)\n",
        "\n",
        "test_x = np.hstack([stock[400:], news[400:]])\n",
        "test_y = pre_stock_data.iloc[400:, 9:].values\n",
        "\n",
        "# make model\n",
        "model=Sequential()\n",
        "# layer 1\n",
        "model.add(Dense(7,input_shape=train_x.shape[1:],activation='linear'))\n",
        "# optimizer if needed\n",
        "# opt=keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
        "opt=keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n",
        "# to make the log of the data collected while training to see in the tensor board\n",
        "time=datetime.now()\n",
        "tbCallBack = keras.callbacks.TensorBoard(log_dir='./log/benchmark_1'+str(time), write_graph=True)\n",
        "# to create the check points of the weights based on the best validation los\n",
        "filepath=\"./log/benchmark_1.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint,tbCallBack]\n",
        "\n",
        "model.summary()\n",
        "model.fit(\n",
        "         train_x\n",
        "        ,train_y\n",
        "        ,epochs=950\n",
        "        ,batch_size=10\n",
        "        ,verbose=1\n",
        "        ,validation_split=0.1\n",
        "        ,callbacks=callbacks_list\n",
        "        )\n",
        "\n",
        "model=load_model('./log/benchmark_1.best.hdf5')\n",
        "fig = plt.figure(figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
        "predict_train=model.predict(train_x)\n",
        "predict_test=model.predict(test_x)\n",
        "a_score=(abs(predict_test-test_y)/test_y).sum()\n",
        "\n",
        "predict_Date = ['2015/12/21', '2015/12/22', '2015/12/23', '2015/12/24', '2015/12/28', '2015/12/29', '2015/12/30']\n",
        "dataArr = np.append(pre_stock_data['Date'], predict_Date)\n",
        "index = [pd.to_datetime(date, format='%Y-%m-%d').date() for date in dataArr]\n",
        "\n",
        "predata = np.squeeze(predict_test[:, :1], axis=1)[0:-6]\n",
        "day1 = np.squeeze(predict_test[:, :1], axis=1)[-1]\n",
        "day2 = np.squeeze(predict_test[:, 1:2], axis=1)[-1]\n",
        "day3 = np.squeeze(predict_test[:1, 2:3], axis=1)[-1]\n",
        "day4 = np.squeeze(predict_test[:1, 3:4], axis=1)[-1]\n",
        "day5 = np.squeeze(predict_test[:1, 4:5], axis=1)[-1]\n",
        "day6 = np.squeeze(predict_test[:1, 5:6], axis=1)[-1]\n",
        "day7 = np.squeeze(predict_test[:1, 6:7], axis=1)[-1]\n",
        "predict_arr = np.round(np.array([day1, day2,day3, day4, day5, day6, day7]), 1)\n",
        "print('predict_seven_days == ', predict_arr)\n",
        "plt.plot(index[400:496],np.squeeze(test_y,axis=1), label='test_y')\n",
        "plt.plot(index[406:496],np.squeeze(predata), label='predict_stock#1')\n",
        "plt.plot(index[496:503],np.squeeze(predict_arr), label='predict_stock#1_seven_days')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/prepare_data_for_modal.py:68: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pre_stock1_data['Month'][i] = int(pre_stock1_data['Date'][i].split('/')[1])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 7)                 63        \n",
            "=================================================================\n",
            "Total params: 63\n",
            "Trainable params: 63\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 360 samples, validate on 40 samples\n",
            "Epoch 1/950\n",
            "360/360 [==============================] - 0s 398us/step - loss: 9560.7944 - acc: 0.1083 - val_loss: 14023.7368 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 14023.73682, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 2/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 9528.0754 - acc: 0.1083 - val_loss: 13973.7112 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00002: val_loss improved from 14023.73682 to 13973.71118, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 3/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 9495.2641 - acc: 0.1083 - val_loss: 13924.1155 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00003: val_loss improved from 13973.71118 to 13924.11548, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 4/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 9462.6461 - acc: 0.1083 - val_loss: 13874.4585 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00004: val_loss improved from 13924.11548 to 13874.45850, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 5/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 9430.1892 - acc: 0.1083 - val_loss: 13824.3853 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00005: val_loss improved from 13874.45850 to 13824.38525, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 6/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 9397.6474 - acc: 0.1083 - val_loss: 13774.9717 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00006: val_loss improved from 13824.38525 to 13774.97168, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 7/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 9365.2472 - acc: 0.1083 - val_loss: 13725.7212 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00007: val_loss improved from 13774.97168 to 13725.72119, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 8/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 9332.8718 - acc: 0.1083 - val_loss: 13676.8914 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00008: val_loss improved from 13725.72119 to 13676.89136, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 9/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 9300.7312 - acc: 0.1083 - val_loss: 13627.3301 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00009: val_loss improved from 13676.89136 to 13627.33008, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 10/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 9268.5003 - acc: 0.1083 - val_loss: 13578.2666 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00010: val_loss improved from 13627.33008 to 13578.26660, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 11/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 9236.3651 - acc: 0.1083 - val_loss: 13529.5012 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00011: val_loss improved from 13578.26660 to 13529.50122, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 12/950\n",
            "360/360 [==============================] - 0s 207us/step - loss: 9204.4171 - acc: 0.1083 - val_loss: 13480.4336 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00012: val_loss improved from 13529.50122 to 13480.43359, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 13/950\n",
            "360/360 [==============================] - 0s 283us/step - loss: 9172.3518 - acc: 0.1083 - val_loss: 13432.3606 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00013: val_loss improved from 13480.43359 to 13432.36060, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 14/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 9140.5297 - acc: 0.1083 - val_loss: 13383.9106 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00014: val_loss improved from 13432.36060 to 13383.91064, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 15/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 9108.7627 - acc: 0.1083 - val_loss: 13335.3630 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00015: val_loss improved from 13383.91064 to 13335.36304, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 16/950\n",
            "360/360 [==============================] - 0s 269us/step - loss: 9077.0764 - acc: 0.1083 - val_loss: 13286.8892 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00016: val_loss improved from 13335.36304 to 13286.88916, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 17/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 9045.4487 - acc: 0.1083 - val_loss: 13238.5969 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00017: val_loss improved from 13286.88916 to 13238.59692, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 18/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 9013.8772 - acc: 0.1083 - val_loss: 13190.7461 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00018: val_loss improved from 13238.59692 to 13190.74609, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 19/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 8982.4777 - acc: 0.1083 - val_loss: 13142.6545 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00019: val_loss improved from 13190.74609 to 13142.65454, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 20/950\n",
            "360/360 [==============================] - 0s 269us/step - loss: 8951.0492 - acc: 0.1083 - val_loss: 13095.0920 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00020: val_loss improved from 13142.65454 to 13095.09204, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 21/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 8919.8092 - acc: 0.1083 - val_loss: 13047.2490 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00021: val_loss improved from 13095.09204 to 13047.24902, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 22/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 8888.4718 - acc: 0.1083 - val_loss: 13000.3096 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00022: val_loss improved from 13047.24902 to 13000.30957, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 23/950\n",
            "360/360 [==============================] - 0s 281us/step - loss: 8857.5175 - acc: 0.1083 - val_loss: 12952.2969 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00023: val_loss improved from 13000.30957 to 12952.29688, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 24/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 8826.3514 - acc: 0.1083 - val_loss: 12905.2051 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00024: val_loss improved from 12952.29688 to 12905.20508, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 25/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 8795.3856 - acc: 0.1083 - val_loss: 12858.0649 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00025: val_loss improved from 12905.20508 to 12858.06494, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 26/950\n",
            "360/360 [==============================] - 0s 289us/step - loss: 8764.4484 - acc: 0.1083 - val_loss: 12811.1763 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00026: val_loss improved from 12858.06494 to 12811.17627, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 27/950\n",
            "360/360 [==============================] - 0s 256us/step - loss: 8733.6300 - acc: 0.1083 - val_loss: 12764.2134 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00027: val_loss improved from 12811.17627 to 12764.21338, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 28/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 8702.8992 - acc: 0.1083 - val_loss: 12717.2742 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00028: val_loss improved from 12764.21338 to 12717.27417, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 29/950\n",
            "360/360 [==============================] - 0s 270us/step - loss: 8672.2072 - acc: 0.1083 - val_loss: 12670.5854 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00029: val_loss improved from 12717.27417 to 12670.58545, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 30/950\n",
            "360/360 [==============================] - 0s 287us/step - loss: 8641.5505 - acc: 0.1083 - val_loss: 12624.2515 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00030: val_loss improved from 12670.58545 to 12624.25146, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 31/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 8611.0739 - acc: 0.1083 - val_loss: 12577.5134 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00031: val_loss improved from 12624.25146 to 12577.51343, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 32/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 8580.5845 - acc: 0.1083 - val_loss: 12531.1033 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00032: val_loss improved from 12577.51343 to 12531.10327, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 33/950\n",
            "360/360 [==============================] - 0s 267us/step - loss: 8550.1550 - acc: 0.1083 - val_loss: 12484.9929 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00033: val_loss improved from 12531.10327 to 12484.99292, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 34/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 8519.9724 - acc: 0.1083 - val_loss: 12438.3699 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00034: val_loss improved from 12484.99292 to 12438.36987, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 35/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 8489.5869 - acc: 0.1083 - val_loss: 12392.7014 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00035: val_loss improved from 12438.36987 to 12392.70142, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 36/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 8459.4350 - acc: 0.1083 - val_loss: 12347.0571 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00036: val_loss improved from 12392.70142 to 12347.05713, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 37/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 8429.3710 - acc: 0.1083 - val_loss: 12301.2590 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00037: val_loss improved from 12347.05713 to 12301.25903, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 38/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 8399.3105 - acc: 0.1083 - val_loss: 12255.8958 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00038: val_loss improved from 12301.25903 to 12255.89575, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 39/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 8369.4803 - acc: 0.1083 - val_loss: 12209.7026 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00039: val_loss improved from 12255.89575 to 12209.70264, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 40/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 8339.3721 - acc: 0.1083 - val_loss: 12165.2493 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00040: val_loss improved from 12209.70264 to 12165.24927, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 41/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 8309.7708 - acc: 0.1083 - val_loss: 12119.3250 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00041: val_loss improved from 12165.24927 to 12119.32495, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 42/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 8280.0243 - acc: 0.1083 - val_loss: 12073.6130 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00042: val_loss improved from 12119.32495 to 12073.61304, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 43/950\n",
            "360/360 [==============================] - 0s 275us/step - loss: 8250.3090 - acc: 0.1083 - val_loss: 12028.4612 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00043: val_loss improved from 12073.61304 to 12028.46118, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 44/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 8220.6058 - acc: 0.1083 - val_loss: 11984.2275 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00044: val_loss improved from 12028.46118 to 11984.22754, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 45/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 8191.2218 - acc: 0.1083 - val_loss: 11939.0144 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00045: val_loss improved from 11984.22754 to 11939.01440, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 46/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 8161.8814 - acc: 0.1083 - val_loss: 11893.3682 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00046: val_loss improved from 11939.01440 to 11893.36816, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 47/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 8132.3151 - acc: 0.1083 - val_loss: 11849.1260 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00047: val_loss improved from 11893.36816 to 11849.12598, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 48/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 8102.9966 - acc: 0.1083 - val_loss: 11805.1050 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00048: val_loss improved from 11849.12598 to 11805.10498, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 49/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 8073.8812 - acc: 0.1083 - val_loss: 11760.3684 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00049: val_loss improved from 11805.10498 to 11760.36841, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 50/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 8044.6202 - acc: 0.1083 - val_loss: 11716.4089 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00050: val_loss improved from 11760.36841 to 11716.40894, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 51/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 8015.6078 - acc: 0.1083 - val_loss: 11671.8481 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00051: val_loss improved from 11716.40894 to 11671.84814, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 52/950\n",
            "360/360 [==============================] - 0s 296us/step - loss: 7986.5626 - acc: 0.1083 - val_loss: 11627.5920 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00052: val_loss improved from 11671.84814 to 11627.59204, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 53/950\n",
            "360/360 [==============================] - 0s 247us/step - loss: 7957.5201 - acc: 0.1083 - val_loss: 11583.9646 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00053: val_loss improved from 11627.59204 to 11583.96460, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 54/950\n",
            "360/360 [==============================] - 0s 274us/step - loss: 7928.7171 - acc: 0.1083 - val_loss: 11539.8606 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00054: val_loss improved from 11583.96460 to 11539.86060, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 55/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 7899.8706 - acc: 0.1083 - val_loss: 11495.8840 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00055: val_loss improved from 11539.86060 to 11495.88403, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 56/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 7871.0361 - acc: 0.1083 - val_loss: 11452.6179 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00056: val_loss improved from 11495.88403 to 11452.61792, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 57/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 7842.4465 - acc: 0.1083 - val_loss: 11408.6248 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00057: val_loss improved from 11452.61792 to 11408.62476, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 58/950\n",
            "360/360 [==============================] - 0s 213us/step - loss: 7813.8003 - acc: 0.1083 - val_loss: 11364.9426 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00058: val_loss improved from 11408.62476 to 11364.94263, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 59/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 7785.2055 - acc: 0.1083 - val_loss: 11321.6160 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00059: val_loss improved from 11364.94263 to 11321.61597, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 60/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 7756.7388 - acc: 0.1083 - val_loss: 11278.1970 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00060: val_loss improved from 11321.61597 to 11278.19702, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 61/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 7728.2043 - acc: 0.1083 - val_loss: 11235.6194 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00061: val_loss improved from 11278.19702 to 11235.61938, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 62/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 7699.9503 - acc: 0.1083 - val_loss: 11192.3665 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00062: val_loss improved from 11235.61938 to 11192.36646, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 63/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 7671.6429 - acc: 0.1083 - val_loss: 11149.2090 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00063: val_loss improved from 11192.36646 to 11149.20898, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 64/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 7643.4476 - acc: 0.1083 - val_loss: 11105.9597 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00064: val_loss improved from 11149.20898 to 11105.95972, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 65/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 7615.2176 - acc: 0.1083 - val_loss: 11063.2085 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00065: val_loss improved from 11105.95972 to 11063.20850, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 66/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 7587.0206 - acc: 0.1083 - val_loss: 11021.0276 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00066: val_loss improved from 11063.20850 to 11021.02759, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 67/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 7559.0602 - acc: 0.1083 - val_loss: 10978.3225 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00067: val_loss improved from 11021.02759 to 10978.32251, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 68/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 7530.9999 - acc: 0.1083 - val_loss: 10936.2170 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00068: val_loss improved from 10978.32251 to 10936.21704, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 69/950\n",
            "360/360 [==============================] - 0s 286us/step - loss: 7503.2352 - acc: 0.1083 - val_loss: 10893.2886 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00069: val_loss improved from 10936.21704 to 10893.28857, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 70/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 7475.2955 - acc: 0.1083 - val_loss: 10850.9751 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00070: val_loss improved from 10893.28857 to 10850.97510, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 71/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 7447.4786 - acc: 0.1083 - val_loss: 10808.9299 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00071: val_loss improved from 10850.97510 to 10808.92993, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 72/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 7419.7719 - acc: 0.1083 - val_loss: 10766.7996 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00072: val_loss improved from 10808.92993 to 10766.79956, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 73/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 7392.1507 - acc: 0.1083 - val_loss: 10724.5071 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00073: val_loss improved from 10766.79956 to 10724.50708, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 74/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 7364.4553 - acc: 0.1083 - val_loss: 10682.7898 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00074: val_loss improved from 10724.50708 to 10682.78979, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 75/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 7336.8721 - acc: 0.1083 - val_loss: 10641.4668 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00075: val_loss improved from 10682.78979 to 10641.46680, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 76/950\n",
            "360/360 [==============================] - 0s 303us/step - loss: 7309.5443 - acc: 0.1083 - val_loss: 10599.0693 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00076: val_loss improved from 10641.46680 to 10599.06934, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 77/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 7282.0229 - acc: 0.1083 - val_loss: 10557.4368 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00077: val_loss improved from 10599.06934 to 10557.43677, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 78/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 7254.6052 - acc: 0.1083 - val_loss: 10516.2427 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00078: val_loss improved from 10557.43677 to 10516.24268, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 79/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 7227.3400 - acc: 0.1083 - val_loss: 10474.7568 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00079: val_loss improved from 10516.24268 to 10474.75684, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 80/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 7200.1170 - acc: 0.1083 - val_loss: 10433.2517 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00080: val_loss improved from 10474.75684 to 10433.25171, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 81/950\n",
            "360/360 [==============================] - 0s 265us/step - loss: 7172.9497 - acc: 0.1083 - val_loss: 10391.8003 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00081: val_loss improved from 10433.25171 to 10391.80029, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 82/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 7145.7116 - acc: 0.1083 - val_loss: 10351.1682 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00082: val_loss improved from 10391.80029 to 10351.16821, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 83/950\n",
            "360/360 [==============================] - 0s 283us/step - loss: 7118.7837 - acc: 0.1083 - val_loss: 10309.7468 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00083: val_loss improved from 10351.16821 to 10309.74683, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 84/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 7091.7145 - acc: 0.1083 - val_loss: 10268.9087 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00084: val_loss improved from 10309.74683 to 10268.90869, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 85/950\n",
            "360/360 [==============================] - 0s 260us/step - loss: 7064.7715 - acc: 0.1083 - val_loss: 10228.1377 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00085: val_loss improved from 10268.90869 to 10228.13770, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 86/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 7037.9579 - acc: 0.1083 - val_loss: 10187.1670 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00086: val_loss improved from 10228.13770 to 10187.16699, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 87/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 7011.0966 - acc: 0.1083 - val_loss: 10146.6240 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00087: val_loss improved from 10187.16699 to 10146.62402, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 88/950\n",
            "360/360 [==============================] - 0s 247us/step - loss: 6984.3827 - acc: 0.1083 - val_loss: 10105.9116 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00088: val_loss improved from 10146.62402 to 10105.91162, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 89/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 6957.6211 - acc: 0.1083 - val_loss: 10065.7156 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00089: val_loss improved from 10105.91162 to 10065.71558, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 90/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 6931.0404 - acc: 0.1083 - val_loss: 10025.1995 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00090: val_loss improved from 10065.71558 to 10025.19946, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 91/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 6904.4815 - acc: 0.1083 - val_loss: 9984.7446 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00091: val_loss improved from 10025.19946 to 9984.74463, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 92/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 6877.9245 - acc: 0.1083 - val_loss: 9944.5623 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00092: val_loss improved from 9984.74463 to 9944.56226, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 93/950\n",
            "360/360 [==============================] - 0s 261us/step - loss: 6851.4565 - acc: 0.1083 - val_loss: 9904.5056 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00093: val_loss improved from 9944.56226 to 9904.50562, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 94/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 6825.0905 - acc: 0.1083 - val_loss: 9864.3469 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00094: val_loss improved from 9904.50562 to 9864.34692, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 95/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 6798.7192 - acc: 0.1083 - val_loss: 9824.4910 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00095: val_loss improved from 9864.34692 to 9824.49097, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 96/950\n",
            "360/360 [==============================] - 0s 274us/step - loss: 6772.4594 - acc: 0.1083 - val_loss: 9784.5715 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00096: val_loss improved from 9824.49097 to 9784.57153, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 97/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 6746.2594 - acc: 0.1083 - val_loss: 9744.7058 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00097: val_loss improved from 9784.57153 to 9744.70581, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 98/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 6720.0905 - acc: 0.1083 - val_loss: 9705.0234 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00098: val_loss improved from 9744.70581 to 9705.02344, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 99/950\n",
            "360/360 [==============================] - 0s 269us/step - loss: 6693.9737 - acc: 0.1083 - val_loss: 9665.5308 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00099: val_loss improved from 9705.02344 to 9665.53076, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 100/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 6667.8896 - acc: 0.1083 - val_loss: 9626.3943 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00100: val_loss improved from 9665.53076 to 9626.39429, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 101/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 6641.9878 - acc: 0.1083 - val_loss: 9586.8142 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00101: val_loss improved from 9626.39429 to 9586.81421, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 102/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 6616.0631 - acc: 0.1083 - val_loss: 9547.3862 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00102: val_loss improved from 9586.81421 to 9547.38623, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 103/950\n",
            "360/360 [==============================] - 0s 278us/step - loss: 6590.1827 - acc: 0.1083 - val_loss: 9508.1772 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00103: val_loss improved from 9547.38623 to 9508.17725, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 104/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 6564.3403 - acc: 0.1083 - val_loss: 9469.3098 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00104: val_loss improved from 9508.17725 to 9469.30981, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 105/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 6538.6152 - acc: 0.1083 - val_loss: 9430.3645 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00105: val_loss improved from 9469.30981 to 9430.36450, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 106/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 6512.9434 - acc: 0.1083 - val_loss: 9391.3535 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00106: val_loss improved from 9430.36450 to 9391.35352, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 107/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 6487.3220 - acc: 0.1083 - val_loss: 9352.4170 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00107: val_loss improved from 9391.35352 to 9352.41699, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 108/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 6461.7638 - acc: 0.1083 - val_loss: 9313.4866 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00108: val_loss improved from 9352.41699 to 9313.48657, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 109/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 6436.1480 - acc: 0.1083 - val_loss: 9275.2949 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00109: val_loss improved from 9313.48657 to 9275.29492, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 110/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 6410.7120 - acc: 0.1083 - val_loss: 9237.0015 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00110: val_loss improved from 9275.29492 to 9237.00146, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 111/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 6385.3918 - acc: 0.1083 - val_loss: 9198.1973 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00111: val_loss improved from 9237.00146 to 9198.19727, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 112/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 6360.0493 - acc: 0.1083 - val_loss: 9159.4512 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00112: val_loss improved from 9198.19727 to 9159.45117, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 113/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 6334.6083 - acc: 0.1083 - val_loss: 9121.8694 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00113: val_loss improved from 9159.45117 to 9121.86938, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 114/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 6309.5361 - acc: 0.1083 - val_loss: 9083.2661 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00114: val_loss improved from 9121.86938 to 9083.26611, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 115/950\n",
            "360/360 [==============================] - 0s 264us/step - loss: 6284.3164 - acc: 0.1083 - val_loss: 9045.0845 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00115: val_loss improved from 9083.26611 to 9045.08447, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 116/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 6259.1637 - acc: 0.1083 - val_loss: 9007.2917 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00116: val_loss improved from 9045.08447 to 9007.29175, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 117/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 6234.2100 - acc: 0.1083 - val_loss: 8968.8931 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00117: val_loss improved from 9007.29175 to 8968.89307, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 118/950\n",
            "360/360 [==============================] - 0s 324us/step - loss: 6209.0591 - acc: 0.1083 - val_loss: 8931.6584 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00118: val_loss improved from 8968.89307 to 8931.65845, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 119/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 6184.2593 - acc: 0.1083 - val_loss: 8893.4729 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00119: val_loss improved from 8931.65845 to 8893.47290, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 120/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 6159.3404 - acc: 0.1083 - val_loss: 8855.7109 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00120: val_loss improved from 8893.47290 to 8855.71094, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 121/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 6134.4881 - acc: 0.1083 - val_loss: 8818.2087 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00121: val_loss improved from 8855.71094 to 8818.20874, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 122/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 6109.7674 - acc: 0.1083 - val_loss: 8780.5323 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00122: val_loss improved from 8818.20874 to 8780.53235, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 123/950\n",
            "360/360 [==============================] - 0s 261us/step - loss: 6085.0575 - acc: 0.1083 - val_loss: 8742.9609 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00123: val_loss improved from 8780.53235 to 8742.96094, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 124/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 6060.2727 - acc: 0.1083 - val_loss: 8706.4148 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00124: val_loss improved from 8742.96094 to 8706.41479, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 125/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 6035.8738 - acc: 0.1083 - val_loss: 8668.4886 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00125: val_loss improved from 8706.41479 to 8668.48865, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 126/950\n",
            "360/360 [==============================] - 0s 209us/step - loss: 6011.1573 - acc: 0.1083 - val_loss: 8631.7762 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00126: val_loss improved from 8668.48865 to 8631.77625, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 127/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 5986.8041 - acc: 0.1083 - val_loss: 8594.2659 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00127: val_loss improved from 8631.77625 to 8594.26587, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 128/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 5962.2268 - acc: 0.1083 - val_loss: 8557.7260 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00128: val_loss improved from 8594.26587 to 8557.72595, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 129/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 5937.9121 - acc: 0.1083 - val_loss: 8520.8381 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00129: val_loss improved from 8557.72595 to 8520.83813, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 130/950\n",
            "360/360 [==============================] - 0s 294us/step - loss: 5913.6010 - acc: 0.1083 - val_loss: 8483.8986 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00130: val_loss improved from 8520.83813 to 8483.89856, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 131/950\n",
            "360/360 [==============================] - 0s 258us/step - loss: 5889.2799 - acc: 0.1083 - val_loss: 8447.4125 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00131: val_loss improved from 8483.89856 to 8447.41248, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 132/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 5865.1762 - acc: 0.1083 - val_loss: 8410.2400 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00132: val_loss improved from 8447.41248 to 8410.23999, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 133/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 5840.9349 - acc: 0.1083 - val_loss: 8373.8489 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00133: val_loss improved from 8410.23999 to 8373.84888, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 134/950\n",
            "360/360 [==============================] - 0s 282us/step - loss: 5816.8161 - acc: 0.1083 - val_loss: 8337.5773 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00134: val_loss improved from 8373.84888 to 8337.57727, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 135/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 5792.8058 - acc: 0.1083 - val_loss: 8301.1853 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00135: val_loss improved from 8337.57727 to 8301.18530, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 136/950\n",
            "360/360 [==============================] - 0s 247us/step - loss: 5768.8201 - acc: 0.1083 - val_loss: 8264.8774 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00136: val_loss improved from 8301.18530 to 8264.87744, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 137/950\n",
            "360/360 [==============================] - 0s 261us/step - loss: 5744.8169 - acc: 0.1083 - val_loss: 8229.0469 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00137: val_loss improved from 8264.87744 to 8229.04688, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 138/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 5721.0053 - acc: 0.1083 - val_loss: 8192.8784 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00138: val_loss improved from 8229.04688 to 8192.87842, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 139/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 5697.1833 - acc: 0.1083 - val_loss: 8156.7770 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00139: val_loss improved from 8192.87842 to 8156.77698, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 140/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 5673.3912 - acc: 0.1083 - val_loss: 8120.9648 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00140: val_loss improved from 8156.77698 to 8120.96484, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 141/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 5649.7272 - acc: 0.1083 - val_loss: 8084.8914 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00141: val_loss improved from 8120.96484 to 8084.89136, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 142/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 5626.0346 - acc: 0.1083 - val_loss: 8049.2426 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00142: val_loss improved from 8084.89136 to 8049.24255, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 143/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 5602.4652 - acc: 0.1083 - val_loss: 8013.4735 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00143: val_loss improved from 8049.24255 to 8013.47351, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 144/950\n",
            "360/360 [==============================] - 0s 208us/step - loss: 5578.9214 - acc: 0.1083 - val_loss: 7977.8887 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00144: val_loss improved from 8013.47351 to 7977.88867, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 145/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 5555.5158 - acc: 0.1083 - val_loss: 7942.0046 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00145: val_loss improved from 7977.88867 to 7942.00464, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 146/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 5531.9089 - acc: 0.1083 - val_loss: 7907.3286 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00146: val_loss improved from 7942.00464 to 7907.32861, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 147/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 5508.7464 - acc: 0.1083 - val_loss: 7871.4374 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00147: val_loss improved from 7907.32861 to 7871.43738, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 148/950\n",
            "360/360 [==============================] - 0s 289us/step - loss: 5485.3252 - acc: 0.1083 - val_loss: 7836.3149 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00148: val_loss improved from 7871.43738 to 7836.31494, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 149/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 5462.0468 - acc: 0.1083 - val_loss: 7801.4011 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00149: val_loss improved from 7836.31494 to 7801.40112, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 150/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 5438.8733 - acc: 0.1083 - val_loss: 7766.3926 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00150: val_loss improved from 7801.40112 to 7766.39258, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 151/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 5415.7635 - acc: 0.1083 - val_loss: 7731.2926 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00151: val_loss improved from 7766.39258 to 7731.29260, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 152/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 5392.6544 - acc: 0.1083 - val_loss: 7696.3683 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00152: val_loss improved from 7731.29260 to 7696.36829, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 153/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 5369.5386 - acc: 0.1083 - val_loss: 7662.0774 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00153: val_loss improved from 7696.36829 to 7662.07739, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 154/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 5346.6867 - acc: 0.1083 - val_loss: 7626.9772 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00154: val_loss improved from 7662.07739 to 7626.97717, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 155/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 5323.7248 - acc: 0.1083 - val_loss: 7592.2512 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00155: val_loss improved from 7626.97717 to 7592.25122, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 156/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 5300.7892 - acc: 0.1083 - val_loss: 7558.1033 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00156: val_loss improved from 7592.25122 to 7558.10327, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 157/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 5278.0925 - acc: 0.1083 - val_loss: 7523.2448 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00157: val_loss improved from 7558.10327 to 7523.24475, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 158/950\n",
            "360/360 [==============================] - 0s 262us/step - loss: 5255.2098 - acc: 0.1083 - val_loss: 7489.3470 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00158: val_loss improved from 7523.24475 to 7489.34705, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 159/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 5232.6187 - acc: 0.1083 - val_loss: 7454.7682 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00159: val_loss improved from 7489.34705 to 7454.76819, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 160/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 5209.8607 - acc: 0.1083 - val_loss: 7421.0734 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00160: val_loss improved from 7454.76819 to 7421.07336, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 161/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 5187.3875 - acc: 0.1083 - val_loss: 7386.5751 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00161: val_loss improved from 7421.07336 to 7386.57507, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 162/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 5164.7983 - acc: 0.1083 - val_loss: 7352.6338 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00162: val_loss improved from 7386.57507 to 7352.63379, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 163/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 5142.3619 - acc: 0.1083 - val_loss: 7318.4834 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00163: val_loss improved from 7352.63379 to 7318.48340, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 164/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 5119.8888 - acc: 0.1083 - val_loss: 7284.7367 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00164: val_loss improved from 7318.48340 to 7284.73669, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 165/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 5097.4858 - acc: 0.1083 - val_loss: 7251.2886 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00165: val_loss improved from 7284.73669 to 7251.28857, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 166/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 5075.2682 - acc: 0.1083 - val_loss: 7217.2664 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00166: val_loss improved from 7251.28857 to 7217.26636, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 167/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 5052.9358 - acc: 0.1083 - val_loss: 7183.8130 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00167: val_loss improved from 7217.26636 to 7183.81299, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 168/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 5030.7164 - acc: 0.1083 - val_loss: 7150.5480 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00168: val_loss improved from 7183.81299 to 7150.54797, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 169/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 5008.6033 - acc: 0.1083 - val_loss: 7116.9852 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00169: val_loss improved from 7150.54797 to 7116.98523, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 170/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 4986.5211 - acc: 0.1083 - val_loss: 7083.4958 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00170: val_loss improved from 7116.98523 to 7083.49585, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 171/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 4964.4165 - acc: 0.1083 - val_loss: 7050.4658 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00171: val_loss improved from 7083.49585 to 7050.46582, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 172/950\n",
            "360/360 [==============================] - 0s 260us/step - loss: 4942.4311 - acc: 0.1083 - val_loss: 7017.5516 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00172: val_loss improved from 7050.46582 to 7017.55164, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 173/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 4920.5454 - acc: 0.1083 - val_loss: 6984.2903 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00173: val_loss improved from 7017.55164 to 6984.29028, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 174/950\n",
            "360/360 [==============================] - 0s 256us/step - loss: 4898.6510 - acc: 0.1083 - val_loss: 6951.2068 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00174: val_loss improved from 6984.29028 to 6951.20679, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 175/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 4876.8237 - acc: 0.1083 - val_loss: 6918.2180 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00175: val_loss improved from 6951.20679 to 6918.21802, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 176/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 4855.0326 - acc: 0.1083 - val_loss: 6885.4307 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00176: val_loss improved from 6918.21802 to 6885.43066, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 177/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 4833.3277 - acc: 0.1083 - val_loss: 6852.6599 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00177: val_loss improved from 6885.43066 to 6852.65991, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 178/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 4811.6088 - acc: 0.1083 - val_loss: 6820.2802 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00178: val_loss improved from 6852.65991 to 6820.28015, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 179/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 4790.0349 - acc: 0.1083 - val_loss: 6787.8230 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00179: val_loss improved from 6820.28015 to 6787.82300, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 180/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 4768.4971 - acc: 0.1083 - val_loss: 6755.2076 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00180: val_loss improved from 6787.82300 to 6755.20764, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 181/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 4746.9746 - acc: 0.1083 - val_loss: 6722.8425 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00181: val_loss improved from 6755.20764 to 6722.84253, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 182/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 4725.4929 - acc: 0.1083 - val_loss: 6690.6989 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00182: val_loss improved from 6722.84253 to 6690.69885, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 183/950\n",
            "360/360 [==============================] - 0s 297us/step - loss: 4704.1526 - acc: 0.1083 - val_loss: 6658.2422 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00183: val_loss improved from 6690.69885 to 6658.24219, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 184/950\n",
            "360/360 [==============================] - 0s 252us/step - loss: 4682.7466 - acc: 0.1083 - val_loss: 6626.3196 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00184: val_loss improved from 6658.24219 to 6626.31958, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 185/950\n",
            "360/360 [==============================] - 0s 247us/step - loss: 4661.4385 - acc: 0.1083 - val_loss: 6594.4133 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00185: val_loss improved from 6626.31958 to 6594.41333, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 186/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 4640.2773 - acc: 0.1083 - val_loss: 6562.0875 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00186: val_loss improved from 6594.41333 to 6562.08752, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 187/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 4619.0584 - acc: 0.1083 - val_loss: 6530.0908 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00187: val_loss improved from 6562.08752 to 6530.09082, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 188/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 4597.9102 - acc: 0.1083 - val_loss: 6498.3145 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00188: val_loss improved from 6530.09082 to 6498.31445, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 189/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 4576.8387 - acc: 0.1083 - val_loss: 6466.5830 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00189: val_loss improved from 6498.31445 to 6466.58301, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 190/950\n",
            "360/360 [==============================] - 0s 272us/step - loss: 4555.8710 - acc: 0.1083 - val_loss: 6434.6489 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00190: val_loss improved from 6466.58301 to 6434.64893, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 191/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 4534.7614 - acc: 0.1083 - val_loss: 6403.7701 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00191: val_loss improved from 6434.64893 to 6403.77014, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 192/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 4514.0317 - acc: 0.1083 - val_loss: 6371.7539 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00192: val_loss improved from 6403.77014 to 6371.75391, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 193/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 4493.0477 - acc: 0.1083 - val_loss: 6340.5787 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00193: val_loss improved from 6371.75391 to 6340.57874, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 194/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 4472.2429 - acc: 0.1083 - val_loss: 6309.5099 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00194: val_loss improved from 6340.57874 to 6309.50989, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 195/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 4451.5916 - acc: 0.1083 - val_loss: 6277.7917 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00195: val_loss improved from 6309.50989 to 6277.79175, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 196/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 4430.8073 - acc: 0.1083 - val_loss: 6246.8320 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00196: val_loss improved from 6277.79175 to 6246.83203, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 197/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 4410.1716 - acc: 0.1083 - val_loss: 6215.8962 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00197: val_loss improved from 6246.83203 to 6215.89624, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 198/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 4389.5976 - acc: 0.1083 - val_loss: 6184.8992 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00198: val_loss improved from 6215.89624 to 6184.89917, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 199/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 4369.0440 - acc: 0.1083 - val_loss: 6154.1106 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00199: val_loss improved from 6184.89917 to 6154.11060, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 200/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 4348.5502 - acc: 0.1083 - val_loss: 6123.4442 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00200: val_loss improved from 6154.11060 to 6123.44421, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 201/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 4328.1649 - acc: 0.1083 - val_loss: 6092.5708 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00201: val_loss improved from 6123.44421 to 6092.57080, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 202/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 4307.8079 - acc: 0.1083 - val_loss: 6061.6458 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00202: val_loss improved from 6092.57080 to 6061.64575, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 203/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 4287.4222 - acc: 0.1083 - val_loss: 6031.2836 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00203: val_loss improved from 6061.64575 to 6031.28357, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 204/950\n",
            "360/360 [==============================] - 0s 297us/step - loss: 4267.1683 - acc: 0.1083 - val_loss: 6000.8613 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00204: val_loss improved from 6031.28357 to 6000.86133, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 205/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 4246.9737 - acc: 0.1083 - val_loss: 5970.4237 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00205: val_loss improved from 6000.86133 to 5970.42371, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 206/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 4226.8115 - acc: 0.1083 - val_loss: 5940.0820 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00206: val_loss improved from 5970.42371 to 5940.08203, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 207/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 4206.6870 - acc: 0.1083 - val_loss: 5909.9956 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00207: val_loss improved from 5940.08203 to 5909.99561, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 208/950\n",
            "360/360 [==============================] - 0s 307us/step - loss: 4186.6767 - acc: 0.1083 - val_loss: 5879.6862 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00208: val_loss improved from 5909.99561 to 5879.68616, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 209/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 4166.5913 - acc: 0.1083 - val_loss: 5850.0156 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00209: val_loss improved from 5879.68616 to 5850.01562, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 210/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 4146.7137 - acc: 0.1083 - val_loss: 5819.9602 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00210: val_loss improved from 5850.01562 to 5819.96021, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 211/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 4126.7957 - acc: 0.1083 - val_loss: 5790.2419 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00211: val_loss improved from 5819.96021 to 5790.24194, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 212/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 4107.0274 - acc: 0.1083 - val_loss: 5760.0537 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00212: val_loss improved from 5790.24194 to 5760.05371, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 213/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 4087.1357 - acc: 0.1083 - val_loss: 5730.6731 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00213: val_loss improved from 5760.05371 to 5730.67310, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 214/950\n",
            "360/360 [==============================] - 0s 277us/step - loss: 4067.4852 - acc: 0.1083 - val_loss: 5700.8173 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00214: val_loss improved from 5730.67310 to 5700.81726, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 215/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 4047.7717 - acc: 0.1083 - val_loss: 5671.3372 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00215: val_loss improved from 5700.81726 to 5671.33716, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 216/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 4028.1628 - acc: 0.1083 - val_loss: 5641.8232 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00216: val_loss improved from 5671.33716 to 5641.82324, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 217/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 4008.5732 - acc: 0.1083 - val_loss: 5612.5549 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00217: val_loss improved from 5641.82324 to 5612.55493, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 218/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 3989.0878 - acc: 0.1083 - val_loss: 5583.2054 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00218: val_loss improved from 5612.55493 to 5583.20544, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 219/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 3969.6694 - acc: 0.1083 - val_loss: 5553.6951 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00219: val_loss improved from 5583.20544 to 5553.69507, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 220/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 3950.2116 - acc: 0.1083 - val_loss: 5524.6957 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00220: val_loss improved from 5553.69507 to 5524.69568, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 221/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 3930.8326 - acc: 0.1083 - val_loss: 5495.9359 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00221: val_loss improved from 5524.69568 to 5495.93591, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 222/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 3911.5989 - acc: 0.1083 - val_loss: 5466.8406 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00222: val_loss improved from 5495.93591 to 5466.84058, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 223/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 3892.3230 - acc: 0.1083 - val_loss: 5438.0515 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00223: val_loss improved from 5466.84058 to 5438.05151, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 224/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 3873.2133 - acc: 0.1083 - val_loss: 5408.8695 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00224: val_loss improved from 5438.05151 to 5408.86951, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 225/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 3853.9651 - acc: 0.1083 - val_loss: 5380.5212 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00225: val_loss improved from 5408.86951 to 5380.52124, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 226/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 3834.9573 - acc: 0.1083 - val_loss: 5351.7843 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00226: val_loss improved from 5380.52124 to 5351.78430, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 227/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 3815.8373 - acc: 0.1083 - val_loss: 5323.7402 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00227: val_loss improved from 5351.78430 to 5323.74023, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 228/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 3797.0229 - acc: 0.1083 - val_loss: 5294.7256 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00228: val_loss improved from 5323.74023 to 5294.72559, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 229/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 3777.9732 - acc: 0.1083 - val_loss: 5266.7306 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00229: val_loss improved from 5294.72559 to 5266.73059, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 230/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 3759.1453 - acc: 0.1083 - val_loss: 5238.4846 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00230: val_loss improved from 5266.73059 to 5238.48462, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 231/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 3740.3151 - acc: 0.1083 - val_loss: 5210.4711 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00231: val_loss improved from 5238.48462 to 5210.47107, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 232/950\n",
            "360/360 [==============================] - 0s 285us/step - loss: 3721.6315 - acc: 0.1083 - val_loss: 5182.0764 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00232: val_loss improved from 5210.47107 to 5182.07642, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 233/950\n",
            "360/360 [==============================] - 0s 298us/step - loss: 3702.8561 - acc: 0.1083 - val_loss: 5154.1899 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00233: val_loss improved from 5182.07642 to 5154.18994, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 234/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 3684.2410 - acc: 0.1083 - val_loss: 5126.1260 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00234: val_loss improved from 5154.18994 to 5126.12598, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 235/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 3665.6874 - acc: 0.1083 - val_loss: 5097.9128 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00235: val_loss improved from 5126.12598 to 5097.91284, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 236/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 3647.0653 - acc: 0.1083 - val_loss: 5070.2997 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00236: val_loss improved from 5097.91284 to 5070.29968, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 237/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 3628.5718 - acc: 0.1083 - val_loss: 5042.8301 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00237: val_loss improved from 5070.29968 to 5042.83008, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 238/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 3610.1650 - acc: 0.1083 - val_loss: 5015.2402 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00238: val_loss improved from 5042.83008 to 5015.24023, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 239/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 3591.8310 - acc: 0.1083 - val_loss: 4987.3739 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00239: val_loss improved from 5015.24023 to 4987.37390, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 240/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 3573.4542 - acc: 0.1083 - val_loss: 4959.9941 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00240: val_loss improved from 4987.37390 to 4959.99414, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 241/950\n",
            "360/360 [==============================] - 0s 258us/step - loss: 3555.1901 - acc: 0.1083 - val_loss: 4932.6528 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00241: val_loss improved from 4959.99414 to 4932.65283, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 242/950\n",
            "360/360 [==============================] - 0s 274us/step - loss: 3536.8913 - acc: 0.1083 - val_loss: 4905.8838 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00242: val_loss improved from 4932.65283 to 4905.88379, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 243/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 3518.8116 - acc: 0.1083 - val_loss: 4878.5652 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00243: val_loss improved from 4905.88379 to 4878.56519, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 244/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 3500.6973 - acc: 0.1083 - val_loss: 4851.3505 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00244: val_loss improved from 4878.56519 to 4851.35046, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 245/950\n",
            "360/360 [==============================] - 0s 267us/step - loss: 3482.5803 - acc: 0.1083 - val_loss: 4824.4951 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00245: val_loss improved from 4851.35046 to 4824.49512, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 246/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 3464.6094 - acc: 0.1083 - val_loss: 4797.4227 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00246: val_loss improved from 4824.49512 to 4797.42273, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 247/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 3446.6206 - acc: 0.1083 - val_loss: 4770.6450 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00247: val_loss improved from 4797.42273 to 4770.64502, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 248/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 3428.7120 - acc: 0.1083 - val_loss: 4743.8942 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00248: val_loss improved from 4770.64502 to 4743.89417, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 249/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 3410.8490 - acc: 0.1083 - val_loss: 4717.2944 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00249: val_loss improved from 4743.89417 to 4717.29443, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 250/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 3393.0691 - acc: 0.1083 - val_loss: 4690.5273 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00250: val_loss improved from 4717.29443 to 4690.52734, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 251/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 3375.3191 - acc: 0.1083 - val_loss: 4663.8715 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00251: val_loss improved from 4690.52734 to 4663.87146, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 252/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 3357.5859 - acc: 0.1083 - val_loss: 4637.5797 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00252: val_loss improved from 4663.87146 to 4637.57971, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 253/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 3339.9411 - acc: 0.1083 - val_loss: 4611.2961 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00253: val_loss improved from 4637.57971 to 4611.29614, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 254/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 3322.4186 - acc: 0.1083 - val_loss: 4584.5928 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00254: val_loss improved from 4611.29614 to 4584.59277, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 255/950\n",
            "360/360 [==============================] - 0s 256us/step - loss: 3304.7885 - acc: 0.1083 - val_loss: 4558.5494 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00255: val_loss improved from 4584.59277 to 4558.54944, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 256/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 3287.3315 - acc: 0.1083 - val_loss: 4532.4388 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00256: val_loss improved from 4558.54944 to 4532.43884, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 257/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 3269.8792 - acc: 0.1083 - val_loss: 4506.4543 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00257: val_loss improved from 4532.43884 to 4506.45435, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 258/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 3252.4949 - acc: 0.1083 - val_loss: 4480.5504 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00258: val_loss improved from 4506.45435 to 4480.55042, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 259/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 3235.1625 - acc: 0.1083 - val_loss: 4454.7136 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00259: val_loss improved from 4480.55042 to 4454.71362, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 260/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 3217.9215 - acc: 0.1083 - val_loss: 4428.6958 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00260: val_loss improved from 4454.71362 to 4428.69580, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 261/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 3200.6344 - acc: 0.1083 - val_loss: 4403.1893 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00261: val_loss improved from 4428.69580 to 4403.18933, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 262/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 3183.5600 - acc: 0.1083 - val_loss: 4377.1248 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00262: val_loss improved from 4403.18933 to 4377.12476, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 263/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 3166.3287 - acc: 0.1083 - val_loss: 4351.8865 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00263: val_loss improved from 4377.12476 to 4351.88647, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 264/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 3149.3323 - acc: 0.1083 - val_loss: 4326.3032 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00264: val_loss improved from 4351.88647 to 4326.30322, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 265/950\n",
            "360/360 [==============================] - 0s 256us/step - loss: 3132.2868 - acc: 0.1083 - val_loss: 4301.0024 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00265: val_loss improved from 4326.30322 to 4301.00244, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 266/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 3115.3663 - acc: 0.1056 - val_loss: 4275.6031 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00266: val_loss improved from 4301.00244 to 4275.60315, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 267/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 3098.3903 - acc: 0.1056 - val_loss: 4250.7415 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00267: val_loss improved from 4275.60315 to 4250.74146, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 268/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 3081.6523 - acc: 0.1083 - val_loss: 4225.2294 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00268: val_loss improved from 4250.74146 to 4225.22943, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 269/950\n",
            "360/360 [==============================] - 0s 209us/step - loss: 3064.7674 - acc: 0.1056 - val_loss: 4200.3586 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00269: val_loss improved from 4225.22943 to 4200.35864, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 270/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 3048.0539 - acc: 0.1056 - val_loss: 4175.3871 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00270: val_loss improved from 4200.35864 to 4175.38708, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 271/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 3031.3904 - acc: 0.1056 - val_loss: 4150.3115 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00271: val_loss improved from 4175.38708 to 4150.31152, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 272/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 3014.6819 - acc: 0.1056 - val_loss: 4125.7634 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00272: val_loss improved from 4150.31152 to 4125.76343, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 273/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 2998.1488 - acc: 0.1056 - val_loss: 4100.9249 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00273: val_loss improved from 4125.76343 to 4100.92487, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 274/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 2981.5956 - acc: 0.1056 - val_loss: 4076.2488 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00274: val_loss improved from 4100.92487 to 4076.24878, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 275/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 2965.1070 - acc: 0.1056 - val_loss: 4051.7021 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00275: val_loss improved from 4076.24878 to 4051.70215, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 276/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 2948.6556 - acc: 0.1056 - val_loss: 4027.3960 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00276: val_loss improved from 4051.70215 to 4027.39600, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 277/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 2932.3368 - acc: 0.1056 - val_loss: 4002.7241 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00277: val_loss improved from 4027.39600 to 4002.72412, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 278/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 2915.9653 - acc: 0.1056 - val_loss: 3978.4286 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00278: val_loss improved from 4002.72412 to 3978.42859, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 279/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 2899.6704 - acc: 0.1056 - val_loss: 3954.3483 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00279: val_loss improved from 3978.42859 to 3954.34827, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 280/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 2883.4621 - acc: 0.1056 - val_loss: 3930.2414 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00280: val_loss improved from 3954.34827 to 3930.24139, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 281/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 2867.3566 - acc: 0.1056 - val_loss: 3905.8445 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00281: val_loss improved from 3930.24139 to 3905.84454, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 282/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 2851.1277 - acc: 0.1056 - val_loss: 3882.1807 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00282: val_loss improved from 3905.84454 to 3882.18073, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 283/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 2835.1559 - acc: 0.1056 - val_loss: 3858.0491 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00283: val_loss improved from 3882.18073 to 3858.04913, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 284/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 2819.0866 - acc: 0.1056 - val_loss: 3834.3570 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00284: val_loss improved from 3858.04913 to 3834.35699, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 285/950\n",
            "360/360 [==============================] - 0s 257us/step - loss: 2803.1420 - acc: 0.1056 - val_loss: 3810.6528 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00285: val_loss improved from 3834.35699 to 3810.65277, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 286/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 2787.2525 - acc: 0.1056 - val_loss: 3786.8847 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00286: val_loss improved from 3810.65277 to 3786.88470, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 287/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 2771.3720 - acc: 0.1056 - val_loss: 3763.3983 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00287: val_loss improved from 3786.88470 to 3763.39825, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 288/950\n",
            "360/360 [==============================] - 0s 285us/step - loss: 2755.5779 - acc: 0.1056 - val_loss: 3739.9309 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00288: val_loss improved from 3763.39825 to 3739.93091, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 289/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 2739.8167 - acc: 0.1056 - val_loss: 3716.5867 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00289: val_loss improved from 3739.93091 to 3716.58673, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 290/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 2724.1340 - acc: 0.1056 - val_loss: 3693.1877 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00290: val_loss improved from 3716.58673 to 3693.18774, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 291/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 2708.5131 - acc: 0.1056 - val_loss: 3669.7352 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00291: val_loss improved from 3693.18774 to 3669.73523, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 292/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 2692.8891 - acc: 0.1056 - val_loss: 3646.5959 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00292: val_loss improved from 3669.73523 to 3646.59589, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 293/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 2677.3195 - acc: 0.1056 - val_loss: 3623.6997 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00293: val_loss improved from 3646.59589 to 3623.69971, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 294/950\n",
            "360/360 [==============================] - 0s 209us/step - loss: 2661.8532 - acc: 0.1056 - val_loss: 3600.7197 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00294: val_loss improved from 3623.69971 to 3600.71973, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 295/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 2646.4458 - acc: 0.1056 - val_loss: 3577.6502 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00295: val_loss improved from 3600.71973 to 3577.65021, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 296/950\n",
            "360/360 [==============================] - 0s 258us/step - loss: 2631.0143 - acc: 0.1056 - val_loss: 3554.9811 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00296: val_loss improved from 3577.65021 to 3554.98114, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 297/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 2615.7188 - acc: 0.1056 - val_loss: 3532.1865 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00297: val_loss improved from 3554.98114 to 3532.18646, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 298/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 2600.4254 - acc: 0.1056 - val_loss: 3509.5281 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00298: val_loss improved from 3532.18646 to 3509.52814, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 299/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 2585.2386 - acc: 0.1056 - val_loss: 3486.6867 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00299: val_loss improved from 3509.52814 to 3486.68671, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 300/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 2570.0586 - acc: 0.1056 - val_loss: 3464.0128 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00300: val_loss improved from 3486.68671 to 3464.01282, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 301/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 2554.8749 - acc: 0.1056 - val_loss: 3441.8815 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00301: val_loss improved from 3464.01282 to 3441.88147, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 302/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 2539.8405 - acc: 0.1056 - val_loss: 3419.5388 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00302: val_loss improved from 3441.88147 to 3419.53882, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 303/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 2524.8276 - acc: 0.1056 - val_loss: 3397.2297 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00303: val_loss improved from 3419.53882 to 3397.22974, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 304/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 2509.8400 - acc: 0.1056 - val_loss: 3375.1674 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00304: val_loss improved from 3397.22974 to 3375.16742, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 305/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 2494.9297 - acc: 0.1056 - val_loss: 3353.1171 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00305: val_loss improved from 3375.16742 to 3353.11707, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 306/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 2480.1302 - acc: 0.1056 - val_loss: 3330.7504 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00306: val_loss improved from 3353.11707 to 3330.75037, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 307/950\n",
            "360/360 [==============================] - 0s 261us/step - loss: 2465.2424 - acc: 0.1056 - val_loss: 3308.9707 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00307: val_loss improved from 3330.75037 to 3308.97070, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 308/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 2450.4978 - acc: 0.1056 - val_loss: 3287.1860 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00308: val_loss improved from 3308.97070 to 3287.18604, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 309/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 2435.8471 - acc: 0.1056 - val_loss: 3265.1547 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00309: val_loss improved from 3287.18604 to 3265.15472, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 310/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 2421.1332 - acc: 0.1056 - val_loss: 3243.6304 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00310: val_loss improved from 3265.15472 to 3243.63037, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 311/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 2406.5263 - acc: 0.1056 - val_loss: 3222.2741 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00311: val_loss improved from 3243.63037 to 3222.27411, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 312/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 2392.0693 - acc: 0.1056 - val_loss: 3200.3539 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00312: val_loss improved from 3222.27411 to 3200.35394, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 313/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 2377.5206 - acc: 0.1056 - val_loss: 3178.9601 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00313: val_loss improved from 3200.35394 to 3178.96014, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 314/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 2363.0662 - acc: 0.1056 - val_loss: 3157.7013 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00314: val_loss improved from 3178.96014 to 3157.70129, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 315/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 2348.7094 - acc: 0.1056 - val_loss: 3136.3281 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00315: val_loss improved from 3157.70129 to 3136.32812, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 316/950\n",
            "360/360 [==============================] - 0s 300us/step - loss: 2334.4127 - acc: 0.1056 - val_loss: 3114.8484 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00316: val_loss improved from 3136.32812 to 3114.84845, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 317/950\n",
            "360/360 [==============================] - 0s 212us/step - loss: 2320.0556 - acc: 0.1056 - val_loss: 3093.9700 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00317: val_loss improved from 3114.84845 to 3093.96997, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 318/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 2305.8547 - acc: 0.1056 - val_loss: 3072.9402 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00318: val_loss improved from 3093.96997 to 3072.94025, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 319/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 2291.6469 - acc: 0.1056 - val_loss: 3052.2704 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00319: val_loss improved from 3072.94025 to 3052.27045, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 320/950\n",
            "360/360 [==============================] - 0s 286us/step - loss: 2277.5804 - acc: 0.1056 - val_loss: 3031.1904 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00320: val_loss improved from 3052.27045 to 3031.19043, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 321/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 2263.5297 - acc: 0.1056 - val_loss: 3010.1387 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00321: val_loss improved from 3031.19043 to 3010.13867, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 322/950\n",
            "360/360 [==============================] - 0s 211us/step - loss: 2249.4477 - acc: 0.1056 - val_loss: 2989.5883 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00322: val_loss improved from 3010.13867 to 2989.58826, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 323/950\n",
            "360/360 [==============================] - 0s 211us/step - loss: 2235.4838 - acc: 0.1056 - val_loss: 2969.0784 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00323: val_loss improved from 2989.58826 to 2969.07843, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 324/950\n",
            "360/360 [==============================] - 0s 213us/step - loss: 2221.6277 - acc: 0.1056 - val_loss: 2948.2819 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00324: val_loss improved from 2969.07843 to 2948.28192, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 325/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 2207.7025 - acc: 0.1056 - val_loss: 2927.9996 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00325: val_loss improved from 2948.28192 to 2927.99963, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 326/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 2193.9242 - acc: 0.1056 - val_loss: 2907.5684 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00326: val_loss improved from 2927.99963 to 2907.56842, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 327/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 2180.1780 - acc: 0.1056 - val_loss: 2887.1652 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00327: val_loss improved from 2907.56842 to 2887.16522, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 328/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 2166.4189 - acc: 0.1056 - val_loss: 2867.1580 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00328: val_loss improved from 2887.16522 to 2867.15802, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 329/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 2152.8017 - acc: 0.1056 - val_loss: 2846.9966 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00329: val_loss improved from 2867.15802 to 2846.99658, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 330/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 2139.1929 - acc: 0.1056 - val_loss: 2826.9265 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00330: val_loss improved from 2846.99658 to 2826.92645, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 331/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 2125.6591 - acc: 0.1056 - val_loss: 2806.8262 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00331: val_loss improved from 2826.92645 to 2806.82623, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 332/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 2112.1234 - acc: 0.1056 - val_loss: 2787.0419 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00332: val_loss improved from 2806.82623 to 2787.04187, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 333/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 2098.6838 - acc: 0.1056 - val_loss: 2767.3026 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00333: val_loss improved from 2787.04187 to 2767.30261, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 334/950\n",
            "360/360 [==============================] - 0s 279us/step - loss: 2085.3200 - acc: 0.1056 - val_loss: 2747.4072 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00334: val_loss improved from 2767.30261 to 2747.40723, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 335/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 2071.9705 - acc: 0.1056 - val_loss: 2727.6606 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00335: val_loss improved from 2747.40723 to 2727.66064, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 336/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 2058.6817 - acc: 0.1056 - val_loss: 2708.0388 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00336: val_loss improved from 2727.66064 to 2708.03882, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 337/950\n",
            "360/360 [==============================] - 0s 267us/step - loss: 2045.4068 - acc: 0.1056 - val_loss: 2688.7513 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00337: val_loss improved from 2708.03882 to 2688.75134, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 338/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 2032.2260 - acc: 0.1056 - val_loss: 2669.5131 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00338: val_loss improved from 2688.75134 to 2669.51312, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 339/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 2019.1584 - acc: 0.1056 - val_loss: 2649.8947 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00339: val_loss improved from 2669.51312 to 2649.89465, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 340/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 2006.0545 - acc: 0.1056 - val_loss: 2630.5334 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00340: val_loss improved from 2649.89465 to 2630.53339, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 341/950\n",
            "360/360 [==============================] - 0s 291us/step - loss: 1992.9869 - acc: 0.1056 - val_loss: 2611.5090 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00341: val_loss improved from 2630.53339 to 2611.50903, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 342/950\n",
            "360/360 [==============================] - 0s 276us/step - loss: 1980.0080 - acc: 0.1056 - val_loss: 2592.5556 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00342: val_loss improved from 2611.50903 to 2592.55560, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 343/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 1967.0844 - acc: 0.1056 - val_loss: 2573.6367 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00343: val_loss improved from 2592.55560 to 2573.63672, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 344/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 1954.2175 - acc: 0.1056 - val_loss: 2554.7083 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00344: val_loss improved from 2573.63672 to 2554.70831, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 345/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 1941.4592 - acc: 0.1056 - val_loss: 2535.4777 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00345: val_loss improved from 2554.70831 to 2535.47772, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 346/950\n",
            "360/360 [==============================] - 0s 262us/step - loss: 1928.5968 - acc: 0.1056 - val_loss: 2516.9261 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00346: val_loss improved from 2535.47772 to 2516.92615, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 347/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 1915.9180 - acc: 0.1056 - val_loss: 2498.1559 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00347: val_loss improved from 2516.92615 to 2498.15594, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 348/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 1903.2311 - acc: 0.1056 - val_loss: 2479.6130 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00348: val_loss improved from 2498.15594 to 2479.61298, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 349/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 1890.6258 - acc: 0.1056 - val_loss: 2461.0211 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00349: val_loss improved from 2479.61298 to 2461.02112, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 350/950\n",
            "360/360 [==============================] - 0s 264us/step - loss: 1878.0134 - acc: 0.1056 - val_loss: 2442.7953 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00350: val_loss improved from 2461.02112 to 2442.79529, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 351/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 1865.5611 - acc: 0.1056 - val_loss: 2424.2302 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00351: val_loss improved from 2442.79529 to 2424.23016, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 352/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 1853.0365 - acc: 0.1056 - val_loss: 2406.1004 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00352: val_loss improved from 2424.23016 to 2406.10040, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 353/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 1840.6563 - acc: 0.1056 - val_loss: 2387.8318 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00353: val_loss improved from 2406.10040 to 2387.83185, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 354/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 1828.2609 - acc: 0.1056 - val_loss: 2369.8416 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00354: val_loss improved from 2387.83185 to 2369.84155, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 355/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 1816.0014 - acc: 0.1056 - val_loss: 2351.5587 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00355: val_loss improved from 2369.84155 to 2351.55865, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 356/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 1803.6964 - acc: 0.1056 - val_loss: 2333.6696 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00356: val_loss improved from 2351.55865 to 2333.66956, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 357/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 1791.4868 - acc: 0.1056 - val_loss: 2315.8232 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00357: val_loss improved from 2333.66956 to 2315.82318, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 358/950\n",
            "360/360 [==============================] - 0s 263us/step - loss: 1779.3515 - acc: 0.1056 - val_loss: 2297.8848 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00358: val_loss improved from 2315.82318 to 2297.88483, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 359/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 1767.2054 - acc: 0.1056 - val_loss: 2280.2489 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00359: val_loss improved from 2297.88483 to 2280.24890, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 360/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 1755.1893 - acc: 0.1056 - val_loss: 2262.4466 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00360: val_loss improved from 2280.24890 to 2262.44659, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 361/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 1743.1245 - acc: 0.1056 - val_loss: 2245.0774 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00361: val_loss improved from 2262.44659 to 2245.07739, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 362/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 1731.2370 - acc: 0.1056 - val_loss: 2227.3803 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00362: val_loss improved from 2245.07739 to 2227.38031, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 363/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 1719.2823 - acc: 0.1056 - val_loss: 2210.0914 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00363: val_loss improved from 2227.38031 to 2210.09137, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 364/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 1707.4685 - acc: 0.1056 - val_loss: 2192.6151 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00364: val_loss improved from 2210.09137 to 2192.61511, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 365/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 1695.6336 - acc: 0.1056 - val_loss: 2175.4481 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00365: val_loss improved from 2192.61511 to 2175.44812, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 366/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 1683.8938 - acc: 0.1056 - val_loss: 2158.2955 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00366: val_loss improved from 2175.44812 to 2158.29553, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 367/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 1672.1889 - acc: 0.1056 - val_loss: 2141.2505 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00367: val_loss improved from 2158.29553 to 2141.25055, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 368/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 1660.5481 - acc: 0.1056 - val_loss: 2124.2343 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00368: val_loss improved from 2141.25055 to 2124.23428, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 369/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 1648.9510 - acc: 0.1056 - val_loss: 2107.3045 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00369: val_loss improved from 2124.23428 to 2107.30453, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 370/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 1637.3760 - acc: 0.1056 - val_loss: 2090.6150 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00370: val_loss improved from 2107.30453 to 2090.61502, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 371/950\n",
            "360/360 [==============================] - 0s 213us/step - loss: 1625.9244 - acc: 0.1056 - val_loss: 2073.6942 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00371: val_loss improved from 2090.61502 to 2073.69418, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 372/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 1614.4567 - acc: 0.1056 - val_loss: 2056.9947 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00372: val_loss improved from 2073.69418 to 2056.99469, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 373/950\n",
            "360/360 [==============================] - 0s 300us/step - loss: 1603.0611 - acc: 0.1056 - val_loss: 2040.3874 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00373: val_loss improved from 2056.99469 to 2040.38736, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 374/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 1591.7527 - acc: 0.1056 - val_loss: 2023.6356 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00374: val_loss improved from 2040.38736 to 2023.63562, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 375/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 1580.4415 - acc: 0.1056 - val_loss: 2007.1255 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00375: val_loss improved from 2023.63562 to 2007.12549, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 376/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 1569.1612 - acc: 0.1056 - val_loss: 1990.9432 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00376: val_loss improved from 2007.12549 to 1990.94324, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 377/950\n",
            "360/360 [==============================] - 0s 252us/step - loss: 1558.0506 - acc: 0.1056 - val_loss: 1974.3981 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00377: val_loss improved from 1990.94324 to 1974.39810, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 378/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 1546.8180 - acc: 0.1056 - val_loss: 1958.5125 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00378: val_loss improved from 1974.39810 to 1958.51251, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 379/950\n",
            "360/360 [==============================] - 0s 260us/step - loss: 1535.8224 - acc: 0.1056 - val_loss: 1942.1485 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00379: val_loss improved from 1958.51251 to 1942.14853, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 380/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 1524.7209 - acc: 0.1056 - val_loss: 1926.2845 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00380: val_loss improved from 1942.14853 to 1926.28455, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 381/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 1513.7892 - acc: 0.1056 - val_loss: 1910.1347 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00381: val_loss improved from 1926.28455 to 1910.13470, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 382/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 1502.8247 - acc: 0.1056 - val_loss: 1894.3058 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00382: val_loss improved from 1910.13470 to 1894.30582, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 383/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 1491.9556 - acc: 0.1056 - val_loss: 1878.4527 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00383: val_loss improved from 1894.30582 to 1878.45267, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 384/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 1481.1134 - acc: 0.1056 - val_loss: 1862.7369 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00384: val_loss improved from 1878.45267 to 1862.73694, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 385/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 1470.3116 - acc: 0.1056 - val_loss: 1847.2324 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00385: val_loss improved from 1862.73694 to 1847.23236, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 386/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 1459.6350 - acc: 0.1056 - val_loss: 1831.4563 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00386: val_loss improved from 1847.23236 to 1831.45630, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 387/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 1448.9055 - acc: 0.1056 - val_loss: 1816.0896 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00387: val_loss improved from 1831.45630 to 1816.08963, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 388/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 1438.2928 - acc: 0.1028 - val_loss: 1800.6448 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00388: val_loss improved from 1816.08963 to 1800.64478, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 389/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 1427.7111 - acc: 0.1028 - val_loss: 1785.2899 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00389: val_loss improved from 1800.64478 to 1785.28989, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 390/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 1417.1788 - acc: 0.1028 - val_loss: 1770.0164 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00390: val_loss improved from 1785.28989 to 1770.01636, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 391/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 1406.7098 - acc: 0.1028 - val_loss: 1754.7517 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00391: val_loss improved from 1770.01636 to 1754.75174, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 392/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 1396.2718 - acc: 0.1028 - val_loss: 1739.6144 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00392: val_loss improved from 1754.75174 to 1739.61444, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 393/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 1385.8785 - acc: 0.1028 - val_loss: 1724.6511 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00393: val_loss improved from 1739.61444 to 1724.65109, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 394/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 1375.5653 - acc: 0.1028 - val_loss: 1709.6521 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00394: val_loss improved from 1724.65109 to 1709.65213, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 395/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 1365.2700 - acc: 0.1028 - val_loss: 1694.8472 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00395: val_loss improved from 1709.65213 to 1694.84720, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 396/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 1355.0512 - acc: 0.1028 - val_loss: 1680.0796 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00396: val_loss improved from 1694.84720 to 1680.07956, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 397/950\n",
            "360/360 [==============================] - 0s 263us/step - loss: 1344.9294 - acc: 0.1028 - val_loss: 1665.0868 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00397: val_loss improved from 1680.07956 to 1665.08682, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 398/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 1334.7337 - acc: 0.1028 - val_loss: 1650.5701 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00398: val_loss improved from 1665.08682 to 1650.57013, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 399/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 1324.6838 - acc: 0.1028 - val_loss: 1635.9961 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00399: val_loss improved from 1650.57013 to 1635.99606, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 400/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 1314.6333 - acc: 0.1028 - val_loss: 1621.6606 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00400: val_loss improved from 1635.99606 to 1621.66055, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 401/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 1304.6991 - acc: 0.1028 - val_loss: 1607.0862 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00401: val_loss improved from 1621.66055 to 1607.08624, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 402/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 1294.7455 - acc: 0.1028 - val_loss: 1592.7822 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00402: val_loss improved from 1607.08624 to 1592.78217, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 403/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 1284.8560 - acc: 0.1028 - val_loss: 1578.6206 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00403: val_loss improved from 1592.78217 to 1578.62064, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 404/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 1275.0543 - acc: 0.1028 - val_loss: 1564.3568 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00404: val_loss improved from 1578.62064 to 1564.35678, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 405/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 1265.2397 - acc: 0.1028 - val_loss: 1550.3676 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00405: val_loss improved from 1564.35678 to 1550.36765, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 406/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 1255.5247 - acc: 0.1028 - val_loss: 1536.3748 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00406: val_loss improved from 1550.36765 to 1536.37479, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 407/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 1245.8585 - acc: 0.1028 - val_loss: 1522.3391 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00407: val_loss improved from 1536.37479 to 1522.33908, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 408/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 1236.2236 - acc: 0.1028 - val_loss: 1508.4054 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00408: val_loss improved from 1522.33908 to 1508.40536, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 409/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 1226.6275 - acc: 0.1028 - val_loss: 1494.6719 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00409: val_loss improved from 1508.40536 to 1494.67188, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 410/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 1217.0938 - acc: 0.1028 - val_loss: 1481.0208 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00410: val_loss improved from 1494.67188 to 1481.02084, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 411/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 1207.6263 - acc: 0.1028 - val_loss: 1467.3691 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00411: val_loss improved from 1481.02084 to 1467.36914, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 412/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 1198.2332 - acc: 0.1028 - val_loss: 1453.5779 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00412: val_loss improved from 1467.36914 to 1453.57785, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 413/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 1188.8191 - acc: 0.1028 - val_loss: 1440.0944 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00413: val_loss improved from 1453.57785 to 1440.09436, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 414/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 1179.4657 - acc: 0.1028 - val_loss: 1426.8352 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00414: val_loss improved from 1440.09436 to 1426.83524, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 415/950\n",
            "360/360 [==============================] - 0s 286us/step - loss: 1170.2116 - acc: 0.1028 - val_loss: 1413.4630 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00415: val_loss improved from 1426.83524 to 1413.46298, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 416/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 1160.9810 - acc: 0.1028 - val_loss: 1400.1963 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00416: val_loss improved from 1413.46298 to 1400.19626, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 417/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 1151.7895 - acc: 0.1028 - val_loss: 1387.0693 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00417: val_loss improved from 1400.19626 to 1387.06927, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 418/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 1142.6670 - acc: 0.1028 - val_loss: 1373.9669 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00418: val_loss improved from 1387.06927 to 1373.96695, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 419/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 1133.5813 - acc: 0.1028 - val_loss: 1360.9348 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00419: val_loss improved from 1373.96695 to 1360.93478, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 420/950\n",
            "360/360 [==============================] - 0s 212us/step - loss: 1124.5265 - acc: 0.1028 - val_loss: 1348.1266 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00420: val_loss improved from 1360.93478 to 1348.12665, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 421/950\n",
            "360/360 [==============================] - 0s 267us/step - loss: 1115.5579 - acc: 0.1028 - val_loss: 1335.2578 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00421: val_loss improved from 1348.12665 to 1335.25781, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 422/950\n",
            "360/360 [==============================] - 0s 212us/step - loss: 1106.6184 - acc: 0.1028 - val_loss: 1322.5240 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00422: val_loss improved from 1335.25781 to 1322.52396, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 423/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 1097.7424 - acc: 0.1028 - val_loss: 1309.6963 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00423: val_loss improved from 1322.52396 to 1309.69626, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 424/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 1088.8868 - acc: 0.1028 - val_loss: 1297.1424 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00424: val_loss improved from 1309.69626 to 1297.14243, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 425/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 1080.1222 - acc: 0.1028 - val_loss: 1284.5082 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00425: val_loss improved from 1297.14243 to 1284.50818, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 426/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 1071.3861 - acc: 0.1028 - val_loss: 1271.9237 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00426: val_loss improved from 1284.50818 to 1271.92374, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 427/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 1062.6511 - acc: 0.1028 - val_loss: 1259.6726 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00427: val_loss improved from 1271.92374 to 1259.67264, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 428/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 1054.0442 - acc: 0.1028 - val_loss: 1247.2839 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00428: val_loss improved from 1259.67264 to 1247.28387, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 429/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 1045.4595 - acc: 0.1028 - val_loss: 1234.9854 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00429: val_loss improved from 1247.28387 to 1234.98544, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 430/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 1036.8976 - acc: 0.1028 - val_loss: 1222.8532 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00430: val_loss improved from 1234.98544 to 1222.85318, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 431/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 1028.3998 - acc: 0.1028 - val_loss: 1210.8262 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00431: val_loss improved from 1222.85318 to 1210.82620, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 432/950\n",
            "360/360 [==============================] - 0s 285us/step - loss: 1019.9799 - acc: 0.1028 - val_loss: 1198.6719 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00432: val_loss improved from 1210.82620 to 1198.67194, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 433/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 1011.5719 - acc: 0.1028 - val_loss: 1186.7087 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00433: val_loss improved from 1198.67194 to 1186.70868, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 434/950\n",
            "360/360 [==============================] - 0s 207us/step - loss: 1003.1984 - acc: 0.1028 - val_loss: 1174.9429 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00434: val_loss improved from 1186.70868 to 1174.94287, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 435/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 994.9321 - acc: 0.1028 - val_loss: 1163.1415 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00435: val_loss improved from 1174.94287 to 1163.14153, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 436/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 986.6955 - acc: 0.1028 - val_loss: 1151.2400 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00436: val_loss improved from 1163.14153 to 1151.24004, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 437/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 978.4564 - acc: 0.1028 - val_loss: 1139.6743 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00437: val_loss improved from 1151.24004 to 1139.67426, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 438/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 970.3065 - acc: 0.1028 - val_loss: 1128.1591 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00438: val_loss improved from 1139.67426 to 1128.15915, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 439/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 962.2259 - acc: 0.1028 - val_loss: 1116.5512 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00439: val_loss improved from 1128.15915 to 1116.55124, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 440/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 954.1469 - acc: 0.1028 - val_loss: 1105.1922 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00440: val_loss improved from 1116.55124 to 1105.19221, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 441/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 946.1489 - acc: 0.1028 - val_loss: 1093.7909 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00441: val_loss improved from 1105.19221 to 1093.79088, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 442/950\n",
            "360/360 [==============================] - 0s 285us/step - loss: 938.1670 - acc: 0.1028 - val_loss: 1082.5866 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00442: val_loss improved from 1093.79088 to 1082.58658, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 443/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 930.2899 - acc: 0.1028 - val_loss: 1071.2612 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00443: val_loss improved from 1082.58658 to 1071.26123, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 444/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 922.3860 - acc: 0.1028 - val_loss: 1060.2335 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00444: val_loss improved from 1071.26123 to 1060.23351, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 445/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 914.5983 - acc: 0.1028 - val_loss: 1049.1199 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00445: val_loss improved from 1060.23351 to 1049.11989, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 446/950\n",
            "360/360 [==============================] - 0s 263us/step - loss: 906.8159 - acc: 0.1028 - val_loss: 1038.1187 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00446: val_loss improved from 1049.11989 to 1038.11868, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 447/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 899.0698 - acc: 0.1028 - val_loss: 1027.3693 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00447: val_loss improved from 1038.11868 to 1027.36932, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 448/950\n",
            "360/360 [==============================] - 0s 263us/step - loss: 891.4359 - acc: 0.1028 - val_loss: 1016.4518 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00448: val_loss improved from 1027.36932 to 1016.45178, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 449/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 883.7740 - acc: 0.1028 - val_loss: 1005.7984 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00449: val_loss improved from 1016.45178 to 1005.79837, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 450/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 876.2195 - acc: 0.1028 - val_loss: 995.0314 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00450: val_loss improved from 1005.79837 to 995.03139, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 451/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 868.6841 - acc: 0.1028 - val_loss: 984.3907 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00451: val_loss improved from 995.03139 to 984.39072, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 452/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 861.2112 - acc: 0.1028 - val_loss: 973.7600 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00452: val_loss improved from 984.39072 to 973.75998, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 453/950\n",
            "360/360 [==============================] - 0s 281us/step - loss: 853.7407 - acc: 0.1028 - val_loss: 963.4000 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00453: val_loss improved from 973.75998 to 963.40001, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 454/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 846.3666 - acc: 0.1028 - val_loss: 953.0699 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00454: val_loss improved from 963.40001 to 953.06989, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 455/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 839.0441 - acc: 0.1028 - val_loss: 942.6794 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00455: val_loss improved from 953.06989 to 942.67935, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 456/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 831.7458 - acc: 0.1028 - val_loss: 932.4080 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00456: val_loss improved from 942.67935 to 932.40804, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 457/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 824.4842 - acc: 0.1028 - val_loss: 922.3563 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00457: val_loss improved from 932.40804 to 922.35628, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 458/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 817.3026 - acc: 0.1028 - val_loss: 912.2229 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00458: val_loss improved from 922.35628 to 912.22289, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 459/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 810.1502 - acc: 0.1028 - val_loss: 902.1896 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00459: val_loss improved from 912.22289 to 902.18961, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 460/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 803.0577 - acc: 0.1028 - val_loss: 892.2134 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00460: val_loss improved from 902.18961 to 892.21341, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 461/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 795.9860 - acc: 0.1028 - val_loss: 882.3560 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00461: val_loss improved from 892.21341 to 882.35605, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 462/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 789.0037 - acc: 0.1028 - val_loss: 872.4770 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00462: val_loss improved from 882.35605 to 872.47697, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 463/950\n",
            "360/360 [==============================] - 0s 281us/step - loss: 781.9853 - acc: 0.1028 - val_loss: 862.9878 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00463: val_loss improved from 872.47697 to 862.98781, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 464/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 775.1178 - acc: 0.1028 - val_loss: 853.2719 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00464: val_loss improved from 862.98781 to 853.27193, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 465/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 768.2514 - acc: 0.1028 - val_loss: 843.6229 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00465: val_loss improved from 853.27193 to 843.62286, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 466/950\n",
            "360/360 [==============================] - 0s 212us/step - loss: 761.4192 - acc: 0.1056 - val_loss: 834.1371 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00466: val_loss improved from 843.62286 to 834.13710, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 467/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 754.6536 - acc: 0.1056 - val_loss: 824.6918 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00467: val_loss improved from 834.13710 to 824.69180, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 468/950\n",
            "360/360 [==============================] - 0s 291us/step - loss: 747.9154 - acc: 0.1056 - val_loss: 815.4055 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00468: val_loss improved from 824.69180 to 815.40552, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 469/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 741.2471 - acc: 0.1056 - val_loss: 806.0844 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00469: val_loss improved from 815.40552 to 806.08438, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 470/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 734.6181 - acc: 0.1056 - val_loss: 796.8630 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00470: val_loss improved from 806.08438 to 796.86302, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 471/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 728.0232 - acc: 0.1056 - val_loss: 787.7415 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00471: val_loss improved from 796.86302 to 787.74147, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 472/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 721.5000 - acc: 0.1056 - val_loss: 778.6023 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00472: val_loss improved from 787.74147 to 778.60231, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 473/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 714.9941 - acc: 0.1056 - val_loss: 769.6359 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00473: val_loss improved from 778.60231 to 769.63591, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 474/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 708.5434 - acc: 0.1056 - val_loss: 760.7735 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00474: val_loss improved from 769.63591 to 760.77347, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 475/950\n",
            "360/360 [==============================] - 0s 257us/step - loss: 702.1678 - acc: 0.1056 - val_loss: 751.8222 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00475: val_loss improved from 760.77347 to 751.82222, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 476/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 695.7978 - acc: 0.1056 - val_loss: 743.0659 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00476: val_loss improved from 751.82222 to 743.06595, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 477/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 689.5081 - acc: 0.1056 - val_loss: 734.3218 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00477: val_loss improved from 743.06595 to 734.32176, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 478/950\n",
            "360/360 [==============================] - 0s 292us/step - loss: 683.2344 - acc: 0.1056 - val_loss: 725.7050 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00478: val_loss improved from 734.32176 to 725.70503, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 479/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 677.0348 - acc: 0.1056 - val_loss: 717.0977 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00479: val_loss improved from 725.70503 to 717.09773, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 480/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 670.8512 - acc: 0.1083 - val_loss: 708.6827 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00480: val_loss improved from 717.09773 to 708.68272, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 481/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 664.7648 - acc: 0.1083 - val_loss: 700.1593 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00481: val_loss improved from 708.68272 to 700.15930, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 482/950\n",
            "360/360 [==============================] - 0s 210us/step - loss: 658.6629 - acc: 0.1083 - val_loss: 691.8813 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00482: val_loss improved from 700.15930 to 691.88127, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 483/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 652.6437 - acc: 0.1083 - val_loss: 683.6166 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00483: val_loss improved from 691.88127 to 683.61662, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 484/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 646.6618 - acc: 0.1083 - val_loss: 675.4506 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00484: val_loss improved from 683.61662 to 675.45059, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 485/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 640.7428 - acc: 0.1083 - val_loss: 667.2361 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00485: val_loss improved from 675.45059 to 667.23607, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 486/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 634.8262 - acc: 0.1083 - val_loss: 659.2352 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00486: val_loss improved from 667.23607 to 659.23520, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 487/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 629.0096 - acc: 0.1083 - val_loss: 651.1843 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00487: val_loss improved from 659.23520 to 651.18432, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 488/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 623.1974 - acc: 0.1083 - val_loss: 643.2556 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00488: val_loss improved from 651.18432 to 643.25557, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 489/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 617.4506 - acc: 0.1083 - val_loss: 635.4108 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00489: val_loss improved from 643.25557 to 635.41081, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 490/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 611.7282 - acc: 0.1083 - val_loss: 627.6725 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00490: val_loss improved from 635.41081 to 627.67247, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 491/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 606.0850 - acc: 0.1083 - val_loss: 619.9414 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00491: val_loss improved from 627.67247 to 619.94138, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 492/950\n",
            "360/360 [==============================] - 0s 208us/step - loss: 600.4573 - acc: 0.1083 - val_loss: 612.3088 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00492: val_loss improved from 619.94138 to 612.30878, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 493/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 594.8766 - acc: 0.1083 - val_loss: 604.7703 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00493: val_loss improved from 612.30878 to 604.77032, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 494/950\n",
            "360/360 [==============================] - 0s 207us/step - loss: 589.3840 - acc: 0.1083 - val_loss: 597.1549 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00494: val_loss improved from 604.77032 to 597.15488, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 495/950\n",
            "360/360 [==============================] - 0s 252us/step - loss: 583.8609 - acc: 0.1083 - val_loss: 589.8106 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00495: val_loss improved from 597.15488 to 589.81062, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 496/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 578.4479 - acc: 0.1083 - val_loss: 582.4043 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00496: val_loss improved from 589.81062 to 582.40434, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 497/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 573.0392 - acc: 0.1083 - val_loss: 575.1188 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00497: val_loss improved from 582.40434 to 575.11877, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 498/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 567.7008 - acc: 0.1083 - val_loss: 567.8701 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00498: val_loss improved from 575.11877 to 567.87006, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 499/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 562.3823 - acc: 0.1083 - val_loss: 560.7693 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00499: val_loss improved from 567.87006 to 560.76931, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 500/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 557.1298 - acc: 0.1083 - val_loss: 553.6708 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00500: val_loss improved from 560.76931 to 553.67075, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 501/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 551.9435 - acc: 0.1083 - val_loss: 546.5133 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00501: val_loss improved from 553.67075 to 546.51326, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 502/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 546.7361 - acc: 0.1083 - val_loss: 539.6122 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00502: val_loss improved from 546.51326 to 539.61216, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 503/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 541.6079 - acc: 0.1083 - val_loss: 532.8070 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00503: val_loss improved from 539.61216 to 532.80698, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 504/950\n",
            "360/360 [==============================] - 0s 262us/step - loss: 536.5559 - acc: 0.1083 - val_loss: 525.8989 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00504: val_loss improved from 532.80698 to 525.89889, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 505/950\n",
            "360/360 [==============================] - 0s 275us/step - loss: 531.4819 - acc: 0.1083 - val_loss: 519.2785 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00505: val_loss improved from 525.89889 to 519.27849, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 506/950\n",
            "360/360 [==============================] - 0s 258us/step - loss: 526.5260 - acc: 0.1083 - val_loss: 512.4965 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00506: val_loss improved from 519.27849 to 512.49650, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 507/950\n",
            "360/360 [==============================] - 0s 323us/step - loss: 521.5564 - acc: 0.1083 - val_loss: 505.8866 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00507: val_loss improved from 512.49650 to 505.88658, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 508/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 516.6379 - acc: 0.1083 - val_loss: 499.4094 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00508: val_loss improved from 505.88658 to 499.40936, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 509/950\n",
            "360/360 [==============================] - 0s 257us/step - loss: 511.8018 - acc: 0.1083 - val_loss: 492.8362 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00509: val_loss improved from 499.40936 to 492.83623, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 510/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 506.9461 - acc: 0.1083 - val_loss: 486.5303 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00510: val_loss improved from 492.83623 to 486.53031, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 511/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 502.2028 - acc: 0.1083 - val_loss: 480.0928 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00511: val_loss improved from 486.53031 to 480.09275, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 512/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 497.4726 - acc: 0.1083 - val_loss: 473.7531 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00512: val_loss improved from 480.09275 to 473.75314, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 513/950\n",
            "360/360 [==============================] - 0s 281us/step - loss: 492.7624 - acc: 0.1083 - val_loss: 467.5613 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00513: val_loss improved from 473.75314 to 467.56135, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 514/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 488.1145 - acc: 0.1083 - val_loss: 461.4598 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00514: val_loss improved from 467.56135 to 461.45979, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 515/950\n",
            "360/360 [==============================] - 0s 264us/step - loss: 483.5080 - acc: 0.1083 - val_loss: 455.4501 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00515: val_loss improved from 461.45979 to 455.45014, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 516/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 478.9743 - acc: 0.1083 - val_loss: 449.3577 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00516: val_loss improved from 455.45014 to 449.35773, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 517/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 474.4266 - acc: 0.1083 - val_loss: 443.4786 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00517: val_loss improved from 449.35773 to 443.47862, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 518/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 469.9784 - acc: 0.1083 - val_loss: 437.5387 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00518: val_loss improved from 443.47862 to 437.53873, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 519/950\n",
            "360/360 [==============================] - 0s 257us/step - loss: 465.5148 - acc: 0.1083 - val_loss: 431.8307 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00519: val_loss improved from 437.53873 to 431.83070, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 520/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 461.1678 - acc: 0.1083 - val_loss: 425.9291 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00520: val_loss improved from 431.83070 to 425.92908, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 521/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 456.7896 - acc: 0.1083 - val_loss: 420.2778 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00521: val_loss improved from 425.92908 to 420.27776, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 522/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 452.4764 - acc: 0.1083 - val_loss: 414.6925 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00522: val_loss improved from 420.27776 to 414.69245, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 523/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 448.2361 - acc: 0.1083 - val_loss: 409.0627 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00523: val_loss improved from 414.69245 to 409.06268, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 524/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 444.0049 - acc: 0.1083 - val_loss: 403.5924 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00524: val_loss improved from 409.06268 to 403.59241, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 525/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 439.8193 - acc: 0.1083 - val_loss: 398.1732 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00525: val_loss improved from 403.59241 to 398.17316, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 526/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 435.6665 - acc: 0.1111 - val_loss: 392.8944 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00526: val_loss improved from 398.17316 to 392.89440, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 527/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 431.6011 - acc: 0.1083 - val_loss: 387.5071 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00527: val_loss improved from 392.89440 to 387.50713, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 528/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 427.5216 - acc: 0.1083 - val_loss: 382.3374 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00528: val_loss improved from 387.50713 to 382.33736, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 529/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 423.5361 - acc: 0.1083 - val_loss: 377.0660 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00529: val_loss improved from 382.33736 to 377.06602, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 530/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 419.5297 - acc: 0.1083 - val_loss: 372.0259 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00530: val_loss improved from 377.06602 to 372.02587, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 531/950\n",
            "360/360 [==============================] - 0s 269us/step - loss: 415.6151 - acc: 0.1083 - val_loss: 366.9588 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00531: val_loss improved from 372.02587 to 366.95884, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 532/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 411.7164 - acc: 0.1083 - val_loss: 362.0138 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00532: val_loss improved from 366.95884 to 362.01380, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 533/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 407.8747 - acc: 0.1083 - val_loss: 357.0433 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00533: val_loss improved from 362.01380 to 357.04332, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 534/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 404.0601 - acc: 0.1083 - val_loss: 352.1761 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00534: val_loss improved from 357.04332 to 352.17609, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 535/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 400.2814 - acc: 0.1083 - val_loss: 347.4162 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00535: val_loss improved from 352.17609 to 347.41619, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 536/950\n",
            "360/360 [==============================] - 0s 292us/step - loss: 396.5505 - acc: 0.1083 - val_loss: 342.6816 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00536: val_loss improved from 347.41619 to 342.68163, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 537/950\n",
            "360/360 [==============================] - 0s 264us/step - loss: 392.8796 - acc: 0.1083 - val_loss: 337.9238 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00537: val_loss improved from 342.68163 to 337.92378, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 538/950\n",
            "360/360 [==============================] - 0s 252us/step - loss: 389.1988 - acc: 0.1083 - val_loss: 333.3825 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00538: val_loss improved from 337.92378 to 333.38248, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 539/950\n",
            "360/360 [==============================] - 0s 252us/step - loss: 385.6058 - acc: 0.1083 - val_loss: 328.7907 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00539: val_loss improved from 333.38248 to 328.79069, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 540/950\n",
            "360/360 [==============================] - 0s 210us/step - loss: 382.0428 - acc: 0.1083 - val_loss: 324.2595 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00540: val_loss improved from 328.79069 to 324.25950, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 541/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 378.4785 - acc: 0.1083 - val_loss: 319.9104 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00541: val_loss improved from 324.25950 to 319.91039, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 542/950\n",
            "360/360 [==============================] - 0s 281us/step - loss: 375.0065 - acc: 0.1083 - val_loss: 315.5193 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00542: val_loss improved from 319.91039 to 315.51935, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 543/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 371.5497 - acc: 0.1083 - val_loss: 311.1807 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00543: val_loss improved from 315.51935 to 311.18073, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 544/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 368.1139 - acc: 0.1083 - val_loss: 307.0110 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00544: val_loss improved from 311.18073 to 307.01096, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 545/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 364.7557 - acc: 0.1083 - val_loss: 302.8080 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00545: val_loss improved from 307.01096 to 302.80798, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 546/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 361.4179 - acc: 0.1083 - val_loss: 298.6278 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00546: val_loss improved from 302.80798 to 298.62778, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 547/950\n",
            "360/360 [==============================] - 0s 261us/step - loss: 358.1242 - acc: 0.1083 - val_loss: 294.5163 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00547: val_loss improved from 298.62778 to 294.51629, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 548/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 354.8491 - acc: 0.1083 - val_loss: 290.5212 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00548: val_loss improved from 294.51629 to 290.52119, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 549/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 351.6431 - acc: 0.1083 - val_loss: 286.5114 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00549: val_loss improved from 290.52119 to 286.51141, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 550/950\n",
            "360/360 [==============================] - 0s 211us/step - loss: 348.4359 - acc: 0.1083 - val_loss: 282.6623 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00550: val_loss improved from 286.51141 to 282.66234, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 551/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 345.2932 - acc: 0.1083 - val_loss: 278.8597 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00551: val_loss improved from 282.66234 to 278.85970, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 552/950\n",
            "360/360 [==============================] - 0s 265us/step - loss: 342.2011 - acc: 0.1083 - val_loss: 275.0211 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00552: val_loss improved from 278.85970 to 275.02106, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 553/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 339.1065 - acc: 0.1111 - val_loss: 271.3397 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00553: val_loss improved from 275.02106 to 271.33969, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 554/950\n",
            "360/360 [==============================] - 0s 203us/step - loss: 336.0817 - acc: 0.1083 - val_loss: 267.6583 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00554: val_loss improved from 271.33969 to 267.65826, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 555/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 333.0835 - acc: 0.1083 - val_loss: 264.0267 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00555: val_loss improved from 267.65826 to 264.02667, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 556/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 330.1218 - acc: 0.1083 - val_loss: 260.4618 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00556: val_loss improved from 264.02667 to 260.46181, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 557/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 327.2018 - acc: 0.1056 - val_loss: 256.9299 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00557: val_loss improved from 260.46181 to 256.92992, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 558/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 324.2972 - acc: 0.1056 - val_loss: 253.5290 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00558: val_loss improved from 256.92992 to 253.52896, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 559/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 321.4511 - acc: 0.1083 - val_loss: 250.1427 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00559: val_loss improved from 253.52896 to 250.14267, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 560/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 318.6607 - acc: 0.1083 - val_loss: 246.7082 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00560: val_loss improved from 250.14267 to 246.70816, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 561/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 315.8419 - acc: 0.1083 - val_loss: 243.4952 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00561: val_loss improved from 246.70816 to 243.49522, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 562/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 313.1193 - acc: 0.1083 - val_loss: 240.2638 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00562: val_loss improved from 243.49522 to 240.26377, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 563/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 310.3957 - acc: 0.1083 - val_loss: 237.1024 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00563: val_loss improved from 240.26377 to 237.10244, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 564/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 307.7184 - acc: 0.1083 - val_loss: 234.0315 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00564: val_loss improved from 237.10244 to 234.03147, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 565/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 305.0934 - acc: 0.1083 - val_loss: 230.9194 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00565: val_loss improved from 234.03147 to 230.91945, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 566/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 302.4839 - acc: 0.1083 - val_loss: 227.9054 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00566: val_loss improved from 230.91945 to 227.90540, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 567/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 299.9209 - acc: 0.1083 - val_loss: 224.8965 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00567: val_loss improved from 227.90540 to 224.89649, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 568/950\n",
            "360/360 [==============================] - 0s 260us/step - loss: 297.3747 - acc: 0.1111 - val_loss: 222.0044 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00568: val_loss improved from 224.89649 to 222.00444, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 569/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 294.8647 - acc: 0.1083 - val_loss: 219.1775 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00569: val_loss improved from 222.00444 to 219.17747, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 570/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 292.4119 - acc: 0.1083 - val_loss: 216.3394 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00570: val_loss improved from 219.17747 to 216.33941, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 571/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 289.9706 - acc: 0.1083 - val_loss: 213.5865 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00571: val_loss improved from 216.33941 to 213.58646, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 572/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 287.5633 - acc: 0.1083 - val_loss: 210.9319 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00572: val_loss improved from 213.58646 to 210.93188, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 573/950\n",
            "360/360 [==============================] - 0s 290us/step - loss: 285.1955 - acc: 0.1083 - val_loss: 208.3169 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00573: val_loss improved from 210.93188 to 208.31685, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 574/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 282.8795 - acc: 0.1139 - val_loss: 205.6475 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00574: val_loss improved from 208.31685 to 205.64754, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 575/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 280.5608 - acc: 0.1139 - val_loss: 203.1003 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00575: val_loss improved from 205.64754 to 203.10034, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 576/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 278.3008 - acc: 0.1167 - val_loss: 200.5672 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00576: val_loss improved from 203.10034 to 200.56717, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 577/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 276.0498 - acc: 0.1167 - val_loss: 198.1348 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00577: val_loss improved from 200.56717 to 198.13475, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 578/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 273.8393 - acc: 0.1167 - val_loss: 195.7493 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00578: val_loss improved from 198.13475 to 195.74927, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 579/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 271.6909 - acc: 0.1222 - val_loss: 193.3184 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00579: val_loss improved from 195.74927 to 193.31841, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 580/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 269.5265 - acc: 0.1222 - val_loss: 191.0259 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00580: val_loss improved from 193.31841 to 191.02587, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 581/950\n",
            "360/360 [==============================] - 0s 285us/step - loss: 267.4109 - acc: 0.1278 - val_loss: 188.7501 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00581: val_loss improved from 191.02587 to 188.75014, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 582/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 265.3270 - acc: 0.1278 - val_loss: 186.5396 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00582: val_loss improved from 188.75014 to 186.53960, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 583/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 263.2724 - acc: 0.1306 - val_loss: 184.3757 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00583: val_loss improved from 186.53960 to 184.37573, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 584/950\n",
            "360/360 [==============================] - 0s 205us/step - loss: 261.2526 - acc: 0.1306 - val_loss: 182.2357 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00584: val_loss improved from 184.37573 to 182.23572, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 585/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 259.2574 - acc: 0.1306 - val_loss: 180.1617 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00585: val_loss improved from 182.23572 to 180.16166, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 586/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 257.2863 - acc: 0.1389 - val_loss: 178.1247 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00586: val_loss improved from 180.16166 to 178.12471, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 587/950\n",
            "360/360 [==============================] - 0s 213us/step - loss: 255.3561 - acc: 0.1417 - val_loss: 176.1186 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00587: val_loss improved from 178.12471 to 176.11865, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 588/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 253.4423 - acc: 0.1444 - val_loss: 174.1807 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00588: val_loss improved from 176.11865 to 174.18069, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 589/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 251.5632 - acc: 0.1444 - val_loss: 172.2632 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00589: val_loss improved from 174.18069 to 172.26322, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 590/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 249.7048 - acc: 0.1444 - val_loss: 170.4175 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00590: val_loss improved from 172.26322 to 170.41747, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 591/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 247.8928 - acc: 0.1444 - val_loss: 168.5750 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00591: val_loss improved from 170.41747 to 168.57500, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 592/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 246.1013 - acc: 0.1472 - val_loss: 166.7481 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00592: val_loss improved from 168.57500 to 166.74813, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 593/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 244.3056 - acc: 0.1472 - val_loss: 165.0630 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00593: val_loss improved from 166.74813 to 165.06295, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 594/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 242.5907 - acc: 0.1472 - val_loss: 163.3261 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00594: val_loss improved from 165.06295 to 163.32610, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 595/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 240.8640 - acc: 0.1500 - val_loss: 161.6779 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00595: val_loss improved from 163.32610 to 161.67787, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 596/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 239.2029 - acc: 0.1500 - val_loss: 159.9624 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00596: val_loss improved from 161.67787 to 159.96242, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 597/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 237.4941 - acc: 0.1500 - val_loss: 158.4590 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00597: val_loss improved from 159.96242 to 158.45895, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 598/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 235.8775 - acc: 0.1528 - val_loss: 156.9184 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00598: val_loss improved from 158.45895 to 156.91835, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 599/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 234.2494 - acc: 0.1528 - val_loss: 155.4391 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00599: val_loss improved from 156.91835 to 155.43908, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 600/950\n",
            "360/360 [==============================] - 0s 291us/step - loss: 232.6825 - acc: 0.1500 - val_loss: 153.9258 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00600: val_loss improved from 155.43908 to 153.92578, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 601/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 231.1143 - acc: 0.1556 - val_loss: 152.4559 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00601: val_loss improved from 153.92578 to 152.45592, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 602/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 229.5599 - acc: 0.1528 - val_loss: 151.0604 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00602: val_loss improved from 152.45592 to 151.06041, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 603/950\n",
            "360/360 [==============================] - 0s 271us/step - loss: 228.0448 - acc: 0.1556 - val_loss: 149.7074 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00603: val_loss improved from 151.06041 to 149.70740, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 604/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 226.5505 - acc: 0.1583 - val_loss: 148.3657 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00604: val_loss improved from 149.70740 to 148.36573, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 605/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 225.0701 - acc: 0.1556 - val_loss: 147.0880 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00605: val_loss improved from 148.36573 to 147.08798, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 606/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 223.6318 - acc: 0.1583 - val_loss: 145.8016 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00606: val_loss improved from 147.08798 to 145.80162, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 607/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 222.1837 - acc: 0.1639 - val_loss: 144.6028 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00607: val_loss improved from 145.80162 to 144.60282, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 608/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 220.7896 - acc: 0.1611 - val_loss: 143.3717 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00608: val_loss improved from 144.60282 to 143.37172, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 609/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 219.4137 - acc: 0.1722 - val_loss: 142.1491 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00609: val_loss improved from 143.37172 to 142.14905, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 610/950\n",
            "360/360 [==============================] - 0s 298us/step - loss: 218.0148 - acc: 0.1778 - val_loss: 141.0507 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00610: val_loss improved from 142.14905 to 141.05068, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 611/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 216.6877 - acc: 0.1750 - val_loss: 139.9315 - val_acc: 0.2250\n",
            "\n",
            "Epoch 00611: val_loss improved from 141.05068 to 139.93147, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 612/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 215.3468 - acc: 0.1750 - val_loss: 138.8810 - val_acc: 0.2250\n",
            "\n",
            "Epoch 00612: val_loss improved from 139.93147 to 138.88096, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 613/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 214.0563 - acc: 0.1778 - val_loss: 137.8009 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00613: val_loss improved from 138.88096 to 137.80090, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 614/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 212.7568 - acc: 0.1722 - val_loss: 136.8003 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00614: val_loss improved from 137.80090 to 136.80030, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 615/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 211.5091 - acc: 0.1722 - val_loss: 135.7714 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00615: val_loss improved from 136.80030 to 135.77139, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 616/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 210.2443 - acc: 0.1750 - val_loss: 134.8150 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00616: val_loss improved from 135.77139 to 134.81498, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 617/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 209.0137 - acc: 0.1778 - val_loss: 133.8837 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00617: val_loss improved from 134.81498 to 133.88370, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 618/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 207.8114 - acc: 0.1833 - val_loss: 132.9455 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00618: val_loss improved from 133.88370 to 132.94549, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 619/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 206.6055 - acc: 0.1861 - val_loss: 132.0691 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00619: val_loss improved from 132.94549 to 132.06909, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 620/950\n",
            "360/360 [==============================] - 0s 256us/step - loss: 205.4166 - acc: 0.1806 - val_loss: 131.2364 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00620: val_loss improved from 132.06909 to 131.23642, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 621/950\n",
            "360/360 [==============================] - 0s 266us/step - loss: 204.2827 - acc: 0.1861 - val_loss: 130.3643 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00621: val_loss improved from 131.23642 to 130.36429, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 622/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 203.1200 - acc: 0.1889 - val_loss: 129.5613 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00622: val_loss improved from 130.36429 to 129.56127, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 623/950\n",
            "360/360 [==============================] - 0s 211us/step - loss: 201.9920 - acc: 0.1889 - val_loss: 128.7659 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00623: val_loss improved from 129.56127 to 128.76590, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 624/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 200.8651 - acc: 0.1944 - val_loss: 128.0392 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00624: val_loss improved from 128.76590 to 128.03923, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 625/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 199.7832 - acc: 0.1944 - val_loss: 127.2816 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00625: val_loss improved from 128.03923 to 127.28164, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 626/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 198.6958 - acc: 0.1944 - val_loss: 126.5481 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00626: val_loss improved from 127.28164 to 126.54806, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 627/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 197.6225 - acc: 0.1972 - val_loss: 125.8575 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00627: val_loss improved from 126.54806 to 125.85747, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 628/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 196.5657 - acc: 0.1972 - val_loss: 125.1964 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00628: val_loss improved from 125.85747 to 125.19638, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 629/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 195.5475 - acc: 0.2028 - val_loss: 124.4895 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00629: val_loss improved from 125.19638 to 124.48946, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 630/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 194.5156 - acc: 0.2056 - val_loss: 123.8187 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00630: val_loss improved from 124.48946 to 123.81866, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 631/950\n",
            "360/360 [==============================] - 0s 267us/step - loss: 193.4894 - acc: 0.2083 - val_loss: 123.2114 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00631: val_loss improved from 123.81866 to 123.21140, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 632/950\n",
            "360/360 [==============================] - 0s 280us/step - loss: 192.4961 - acc: 0.2111 - val_loss: 122.6067 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00632: val_loss improved from 123.21140 to 122.60669, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 633/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 191.4937 - acc: 0.2139 - val_loss: 122.0647 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00633: val_loss improved from 122.60669 to 122.06465, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 634/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 190.5700 - acc: 0.2139 - val_loss: 121.4342 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00634: val_loss improved from 122.06465 to 121.43417, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 635/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 189.5732 - acc: 0.2139 - val_loss: 120.8981 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00635: val_loss improved from 121.43417 to 120.89805, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 636/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 188.6266 - acc: 0.2167 - val_loss: 120.3797 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00636: val_loss improved from 120.89805 to 120.37970, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 637/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 187.7101 - acc: 0.2167 - val_loss: 119.8299 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00637: val_loss improved from 120.37970 to 119.82985, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 638/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 186.7674 - acc: 0.2194 - val_loss: 119.3453 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00638: val_loss improved from 119.82985 to 119.34530, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 639/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 185.8784 - acc: 0.2194 - val_loss: 118.8375 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00639: val_loss improved from 119.34530 to 118.83746, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 640/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 184.9616 - acc: 0.2167 - val_loss: 118.3764 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00640: val_loss improved from 118.83746 to 118.37639, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 641/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 184.0820 - acc: 0.2194 - val_loss: 117.9095 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00641: val_loss improved from 118.37639 to 117.90949, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 642/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 183.2081 - acc: 0.2222 - val_loss: 117.4510 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00642: val_loss improved from 117.90949 to 117.45102, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 643/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 182.3192 - acc: 0.2250 - val_loss: 117.0530 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00643: val_loss improved from 117.45102 to 117.05295, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 644/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 181.5035 - acc: 0.2250 - val_loss: 116.5894 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00644: val_loss improved from 117.05295 to 116.58936, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 645/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 180.6241 - acc: 0.2250 - val_loss: 116.1958 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00645: val_loss improved from 116.58936 to 116.19578, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 646/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 179.8085 - acc: 0.2278 - val_loss: 115.7949 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00646: val_loss improved from 116.19578 to 115.79494, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 647/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 178.9711 - acc: 0.2278 - val_loss: 115.4096 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00647: val_loss improved from 115.79494 to 115.40965, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 648/950\n",
            "360/360 [==============================] - 0s 327us/step - loss: 178.1532 - acc: 0.2278 - val_loss: 115.0488 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00648: val_loss improved from 115.40965 to 115.04882, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 649/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 177.3585 - acc: 0.2250 - val_loss: 114.6879 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00649: val_loss improved from 115.04882 to 114.68794, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 650/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 176.5500 - acc: 0.2250 - val_loss: 114.3460 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00650: val_loss improved from 114.68794 to 114.34602, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 651/950\n",
            "360/360 [==============================] - 0s 293us/step - loss: 175.7824 - acc: 0.2278 - val_loss: 113.9862 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00651: val_loss improved from 114.34602 to 113.98615, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 652/950\n",
            "360/360 [==============================] - 0s 275us/step - loss: 174.9923 - acc: 0.2306 - val_loss: 113.6585 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00652: val_loss improved from 113.98615 to 113.65851, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 653/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 174.2151 - acc: 0.2306 - val_loss: 113.3491 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00653: val_loss improved from 113.65851 to 113.34914, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 654/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 173.4797 - acc: 0.2306 - val_loss: 113.0282 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00654: val_loss improved from 113.34914 to 113.02819, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 655/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 172.7022 - acc: 0.2333 - val_loss: 112.7413 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00655: val_loss improved from 113.02819 to 112.74129, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 656/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 171.9705 - acc: 0.2333 - val_loss: 112.4378 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00656: val_loss improved from 112.74129 to 112.43775, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 657/950\n",
            "360/360 [==============================] - 0s 262us/step - loss: 171.2456 - acc: 0.2333 - val_loss: 112.1476 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00657: val_loss improved from 112.43775 to 112.14756, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 658/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 170.4914 - acc: 0.2389 - val_loss: 111.8851 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00658: val_loss improved from 112.14756 to 111.88505, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 659/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 169.7844 - acc: 0.2389 - val_loss: 111.6174 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00659: val_loss improved from 111.88505 to 111.61738, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 660/950\n",
            "360/360 [==============================] - 0s 247us/step - loss: 169.0720 - acc: 0.2389 - val_loss: 111.3549 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00660: val_loss improved from 111.61738 to 111.35487, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 661/950\n",
            "360/360 [==============================] - 0s 309us/step - loss: 168.3578 - acc: 0.2389 - val_loss: 111.1102 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00661: val_loss improved from 111.35487 to 111.11016, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 662/950\n",
            "360/360 [==============================] - 0s 262us/step - loss: 167.6676 - acc: 0.2417 - val_loss: 110.8608 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00662: val_loss improved from 111.11016 to 110.86081, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 663/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 166.9781 - acc: 0.2417 - val_loss: 110.6166 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00663: val_loss improved from 110.86081 to 110.61660, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 664/950\n",
            "360/360 [==============================] - 0s 252us/step - loss: 166.2946 - acc: 0.2417 - val_loss: 110.3883 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00664: val_loss improved from 110.61660 to 110.38833, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 665/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 165.6092 - acc: 0.2417 - val_loss: 110.1635 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00665: val_loss improved from 110.38833 to 110.16347, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 666/950\n",
            "360/360 [==============================] - 0s 205us/step - loss: 164.9440 - acc: 0.2417 - val_loss: 109.9515 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00666: val_loss improved from 110.16347 to 109.95147, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 667/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 164.2858 - acc: 0.2417 - val_loss: 109.7390 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00667: val_loss improved from 109.95147 to 109.73902, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 668/950\n",
            "360/360 [==============================] - 0s 280us/step - loss: 163.6179 - acc: 0.2417 - val_loss: 109.5229 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00668: val_loss improved from 109.73902 to 109.52288, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 669/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 162.9799 - acc: 0.2444 - val_loss: 109.3239 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00669: val_loss improved from 109.52288 to 109.32392, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 670/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 162.3319 - acc: 0.2444 - val_loss: 109.1204 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00670: val_loss improved from 109.32392 to 109.12041, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 671/950\n",
            "360/360 [==============================] - 0s 284us/step - loss: 161.6953 - acc: 0.2444 - val_loss: 108.9283 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00671: val_loss improved from 109.12041 to 108.92827, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 672/950\n",
            "360/360 [==============================] - 0s 263us/step - loss: 161.0559 - acc: 0.2444 - val_loss: 108.7499 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00672: val_loss improved from 108.92827 to 108.74992, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 673/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 160.4499 - acc: 0.2444 - val_loss: 108.5705 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00673: val_loss improved from 108.74992 to 108.57054, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 674/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 159.8201 - acc: 0.2528 - val_loss: 108.3846 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00674: val_loss improved from 108.57054 to 108.38459, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 675/950\n",
            "360/360 [==============================] - 0s 266us/step - loss: 159.2097 - acc: 0.2500 - val_loss: 108.2135 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00675: val_loss improved from 108.38459 to 108.21352, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 676/950\n",
            "360/360 [==============================] - 0s 209us/step - loss: 158.6038 - acc: 0.2500 - val_loss: 108.0484 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00676: val_loss improved from 108.21352 to 108.04843, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 677/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 157.9986 - acc: 0.2500 - val_loss: 107.8736 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00677: val_loss improved from 108.04843 to 107.87357, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 678/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 157.4246 - acc: 0.2500 - val_loss: 107.7067 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00678: val_loss improved from 107.87357 to 107.70675, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 679/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 156.8175 - acc: 0.2500 - val_loss: 107.5652 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00679: val_loss improved from 107.70675 to 107.56524, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 680/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 156.2542 - acc: 0.2500 - val_loss: 107.4065 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00680: val_loss improved from 107.56524 to 107.40646, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 681/950\n",
            "360/360 [==============================] - 0s 258us/step - loss: 155.6622 - acc: 0.2500 - val_loss: 107.2253 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00681: val_loss improved from 107.40646 to 107.22530, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 682/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 155.0972 - acc: 0.2528 - val_loss: 107.1005 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00682: val_loss improved from 107.22530 to 107.10052, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 683/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 154.5333 - acc: 0.2528 - val_loss: 106.9632 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00683: val_loss improved from 107.10052 to 106.96322, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 684/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 153.9779 - acc: 0.2556 - val_loss: 106.8041 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00684: val_loss improved from 106.96322 to 106.80412, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 685/950\n",
            "360/360 [==============================] - 0s 210us/step - loss: 153.4133 - acc: 0.2556 - val_loss: 106.6706 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00685: val_loss improved from 106.80412 to 106.67058, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 686/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 152.8696 - acc: 0.2556 - val_loss: 106.5337 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00686: val_loss improved from 106.67058 to 106.53372, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 687/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 152.3219 - acc: 0.2556 - val_loss: 106.3882 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00687: val_loss improved from 106.53372 to 106.38821, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 688/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 151.7908 - acc: 0.2556 - val_loss: 106.2803 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00688: val_loss improved from 106.38821 to 106.28026, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 689/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 151.2519 - acc: 0.2556 - val_loss: 106.1336 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00689: val_loss improved from 106.28026 to 106.13363, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 690/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 150.7256 - acc: 0.2556 - val_loss: 105.9689 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00690: val_loss improved from 106.13363 to 105.96888, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 691/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 150.2007 - acc: 0.2556 - val_loss: 105.8693 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00691: val_loss improved from 105.96888 to 105.86931, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 692/950\n",
            "360/360 [==============================] - 0s 286us/step - loss: 149.6846 - acc: 0.2556 - val_loss: 105.7395 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00692: val_loss improved from 105.86931 to 105.73953, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 693/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 149.1628 - acc: 0.2583 - val_loss: 105.5912 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00693: val_loss improved from 105.73953 to 105.59119, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 694/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 148.6543 - acc: 0.2583 - val_loss: 105.4785 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00694: val_loss improved from 105.59119 to 105.47846, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 695/950\n",
            "360/360 [==============================] - 0s 213us/step - loss: 148.1528 - acc: 0.2583 - val_loss: 105.3388 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00695: val_loss improved from 105.47846 to 105.33875, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 696/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 147.6553 - acc: 0.2583 - val_loss: 105.2320 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00696: val_loss improved from 105.33875 to 105.23195, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 697/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 147.1485 - acc: 0.2583 - val_loss: 105.1149 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00697: val_loss improved from 105.23195 to 105.11489, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 698/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 146.6566 - acc: 0.2583 - val_loss: 104.9793 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00698: val_loss improved from 105.11489 to 104.97934, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 699/950\n",
            "360/360 [==============================] - 0s 266us/step - loss: 146.1673 - acc: 0.2583 - val_loss: 104.8461 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00699: val_loss improved from 104.97934 to 104.84606, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 700/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 145.6810 - acc: 0.2583 - val_loss: 104.7303 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00700: val_loss improved from 104.84606 to 104.73032, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 701/950\n",
            "360/360 [==============================] - 0s 261us/step - loss: 145.1973 - acc: 0.2583 - val_loss: 104.5888 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00701: val_loss improved from 104.73032 to 104.58882, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 702/950\n",
            "360/360 [==============================] - 0s 218us/step - loss: 144.7219 - acc: 0.2583 - val_loss: 104.4694 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00702: val_loss improved from 104.58882 to 104.46939, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 703/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 144.2440 - acc: 0.2583 - val_loss: 104.3569 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00703: val_loss improved from 104.46939 to 104.35690, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 704/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 143.7822 - acc: 0.2583 - val_loss: 104.2236 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00704: val_loss improved from 104.35690 to 104.22356, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 705/950\n",
            "360/360 [==============================] - 0s 287us/step - loss: 143.3110 - acc: 0.2583 - val_loss: 104.1221 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00705: val_loss improved from 104.22356 to 104.12208, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 706/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 142.8295 - acc: 0.2583 - val_loss: 103.9746 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00706: val_loss improved from 104.12208 to 103.97459, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 707/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 142.3861 - acc: 0.2583 - val_loss: 103.8672 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00707: val_loss improved from 103.97459 to 103.86724, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 708/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 141.9121 - acc: 0.2583 - val_loss: 103.7009 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00708: val_loss improved from 103.86724 to 103.70091, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 709/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 141.4729 - acc: 0.2583 - val_loss: 103.6265 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00709: val_loss improved from 103.70091 to 103.62655, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 710/950\n",
            "360/360 [==============================] - 0s 264us/step - loss: 141.0166 - acc: 0.2556 - val_loss: 103.4910 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00710: val_loss improved from 103.62655 to 103.49102, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 711/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 140.5559 - acc: 0.2556 - val_loss: 103.3358 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00711: val_loss improved from 103.49102 to 103.33576, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 712/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 140.1116 - acc: 0.2556 - val_loss: 103.1893 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00712: val_loss improved from 103.33576 to 103.18930, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 713/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 139.6798 - acc: 0.2583 - val_loss: 103.0895 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00713: val_loss improved from 103.18930 to 103.08952, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 714/950\n",
            "360/360 [==============================] - 0s 294us/step - loss: 139.2226 - acc: 0.2611 - val_loss: 102.9312 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00714: val_loss improved from 103.08952 to 102.93123, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 715/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 138.7975 - acc: 0.2611 - val_loss: 102.8525 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00715: val_loss improved from 102.93123 to 102.85254, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 716/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 138.3532 - acc: 0.2611 - val_loss: 102.6731 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00716: val_loss improved from 102.85254 to 102.67312, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 717/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 137.9209 - acc: 0.2611 - val_loss: 102.5370 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00717: val_loss improved from 102.67312 to 102.53695, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 718/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 137.4965 - acc: 0.2611 - val_loss: 102.4114 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00718: val_loss improved from 102.53695 to 102.41144, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 719/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 137.0677 - acc: 0.2611 - val_loss: 102.2629 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00719: val_loss improved from 102.41144 to 102.26288, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 720/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 136.6375 - acc: 0.2639 - val_loss: 102.0924 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00720: val_loss improved from 102.26288 to 102.09244, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 721/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 136.2323 - acc: 0.2611 - val_loss: 102.0328 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00721: val_loss improved from 102.09244 to 102.03280, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 722/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 135.7903 - acc: 0.2611 - val_loss: 101.8788 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00722: val_loss improved from 102.03280 to 101.87884, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 723/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 135.3802 - acc: 0.2611 - val_loss: 101.7233 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00723: val_loss improved from 101.87884 to 101.72325, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 724/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 134.9755 - acc: 0.2611 - val_loss: 101.6125 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00724: val_loss improved from 101.72325 to 101.61250, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 725/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 134.5462 - acc: 0.2611 - val_loss: 101.4211 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00725: val_loss improved from 101.61250 to 101.42109, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 726/950\n",
            "360/360 [==============================] - 0s 293us/step - loss: 134.1483 - acc: 0.2611 - val_loss: 101.3148 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00726: val_loss improved from 101.42109 to 101.31481, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 727/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 133.7245 - acc: 0.2556 - val_loss: 101.1417 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00727: val_loss improved from 101.31481 to 101.14166, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 728/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 133.3337 - acc: 0.2528 - val_loss: 100.9720 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00728: val_loss improved from 101.14166 to 100.97201, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 729/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 132.9243 - acc: 0.2528 - val_loss: 100.8503 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00729: val_loss improved from 100.97201 to 100.85030, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 730/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 132.5166 - acc: 0.2472 - val_loss: 100.7088 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00730: val_loss improved from 100.85030 to 100.70879, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 731/950\n",
            "360/360 [==============================] - 0s 306us/step - loss: 132.1232 - acc: 0.2500 - val_loss: 100.5545 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00731: val_loss improved from 100.70879 to 100.55447, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 732/950\n",
            "360/360 [==============================] - 0s 276us/step - loss: 131.7351 - acc: 0.2500 - val_loss: 100.3619 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00732: val_loss improved from 100.55447 to 100.36188, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 733/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 131.3315 - acc: 0.2500 - val_loss: 100.2558 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00733: val_loss improved from 100.36188 to 100.25582, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 734/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 130.9336 - acc: 0.2528 - val_loss: 100.1129 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00734: val_loss improved from 100.25582 to 100.11293, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 735/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 130.5383 - acc: 0.2556 - val_loss: 99.9183 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00735: val_loss improved from 100.11293 to 99.91835, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 736/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 130.1519 - acc: 0.2556 - val_loss: 99.7958 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00736: val_loss improved from 99.91835 to 99.79579, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 737/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 129.7596 - acc: 0.2528 - val_loss: 99.5943 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00737: val_loss improved from 99.79579 to 99.59428, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 738/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 129.3834 - acc: 0.2528 - val_loss: 99.4880 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00738: val_loss improved from 99.59428 to 99.48799, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 739/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 128.9920 - acc: 0.2556 - val_loss: 99.3050 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00739: val_loss improved from 99.48799 to 99.30505, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 740/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 128.6115 - acc: 0.2556 - val_loss: 99.1106 - val_acc: 0.1750\n",
            "\n",
            "Epoch 00740: val_loss improved from 99.30505 to 99.11060, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 741/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 128.2273 - acc: 0.2556 - val_loss: 98.9603 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00741: val_loss improved from 99.11060 to 98.96034, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 742/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 127.8529 - acc: 0.2556 - val_loss: 98.8094 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00742: val_loss improved from 98.96034 to 98.80944, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 743/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 127.4716 - acc: 0.2556 - val_loss: 98.6301 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00743: val_loss improved from 98.80944 to 98.63015, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 744/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 127.1073 - acc: 0.2583 - val_loss: 98.4766 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00744: val_loss improved from 98.63015 to 98.47661, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 745/950\n",
            "360/360 [==============================] - 0s 211us/step - loss: 126.7348 - acc: 0.2611 - val_loss: 98.3788 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00745: val_loss improved from 98.47661 to 98.37882, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 746/950\n",
            "360/360 [==============================] - 0s 276us/step - loss: 126.3504 - acc: 0.2611 - val_loss: 98.1150 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00746: val_loss improved from 98.37882 to 98.11497, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 747/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 125.9861 - acc: 0.2611 - val_loss: 97.9478 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00747: val_loss improved from 98.11497 to 97.94775, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 748/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 125.6207 - acc: 0.2611 - val_loss: 97.8210 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00748: val_loss improved from 97.94775 to 97.82096, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 749/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 125.2537 - acc: 0.2611 - val_loss: 97.6725 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00749: val_loss improved from 97.82096 to 97.67249, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 750/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 124.8790 - acc: 0.2611 - val_loss: 97.4507 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00750: val_loss improved from 97.67249 to 97.45074, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 751/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 124.5194 - acc: 0.2611 - val_loss: 97.2518 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00751: val_loss improved from 97.45074 to 97.25175, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 752/950\n",
            "360/360 [==============================] - 0s 274us/step - loss: 124.1587 - acc: 0.2639 - val_loss: 97.1223 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00752: val_loss improved from 97.25175 to 97.12234, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 753/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 123.7929 - acc: 0.2639 - val_loss: 96.9345 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00753: val_loss improved from 97.12234 to 96.93452, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 754/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 123.4360 - acc: 0.2611 - val_loss: 96.7327 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00754: val_loss improved from 96.93452 to 96.73268, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 755/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 123.0768 - acc: 0.2639 - val_loss: 96.5452 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00755: val_loss improved from 96.73268 to 96.54522, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 756/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 122.7178 - acc: 0.2583 - val_loss: 96.3913 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00756: val_loss improved from 96.54522 to 96.39132, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 757/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 122.3657 - acc: 0.2583 - val_loss: 96.2355 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00757: val_loss improved from 96.39132 to 96.23553, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 758/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 122.0063 - acc: 0.2583 - val_loss: 96.0651 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00758: val_loss improved from 96.23553 to 96.06512, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 759/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 121.6515 - acc: 0.2583 - val_loss: 95.8264 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00759: val_loss improved from 96.06512 to 95.82645, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 760/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 121.3006 - acc: 0.2583 - val_loss: 95.6234 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00760: val_loss improved from 95.82645 to 95.62340, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 761/950\n",
            "360/360 [==============================] - 0s 213us/step - loss: 120.9479 - acc: 0.2583 - val_loss: 95.4949 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00761: val_loss improved from 95.62340 to 95.49491, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 762/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 120.5967 - acc: 0.2583 - val_loss: 95.2307 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00762: val_loss improved from 95.49491 to 95.23071, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 763/950\n",
            "360/360 [==============================] - 0s 282us/step - loss: 120.2455 - acc: 0.2583 - val_loss: 95.0943 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00763: val_loss improved from 95.23071 to 95.09433, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 764/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 119.8992 - acc: 0.2583 - val_loss: 94.9108 - val_acc: 0.1500\n",
            "\n",
            "Epoch 00764: val_loss improved from 95.09433 to 94.91077, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 765/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 119.5560 - acc: 0.2583 - val_loss: 94.7105 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00765: val_loss improved from 94.91077 to 94.71048, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 766/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 119.2048 - acc: 0.2583 - val_loss: 94.5641 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00766: val_loss improved from 94.71048 to 94.56412, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 767/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 118.8586 - acc: 0.2583 - val_loss: 94.3366 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00767: val_loss improved from 94.56412 to 94.33656, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 768/950\n",
            "360/360 [==============================] - 0s 257us/step - loss: 118.5184 - acc: 0.2583 - val_loss: 94.1810 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00768: val_loss improved from 94.33656 to 94.18102, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 769/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 118.1691 - acc: 0.2583 - val_loss: 94.0052 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00769: val_loss improved from 94.18102 to 94.00525, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 770/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 117.8322 - acc: 0.2583 - val_loss: 93.8205 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00770: val_loss improved from 94.00525 to 93.82053, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 771/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 117.4976 - acc: 0.2611 - val_loss: 93.6892 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00771: val_loss improved from 93.82053 to 93.68924, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 772/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 117.1466 - acc: 0.2611 - val_loss: 93.3816 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00772: val_loss improved from 93.68924 to 93.38161, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 773/950\n",
            "360/360 [==============================] - 0s 289us/step - loss: 116.8327 - acc: 0.2611 - val_loss: 93.3051 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00773: val_loss improved from 93.38161 to 93.30512, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 774/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 116.4727 - acc: 0.2611 - val_loss: 93.0366 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00774: val_loss improved from 93.30512 to 93.03657, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 775/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 116.1393 - acc: 0.2611 - val_loss: 92.8242 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00775: val_loss improved from 93.03657 to 92.82422, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 776/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 115.8079 - acc: 0.2611 - val_loss: 92.6546 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00776: val_loss improved from 92.82422 to 92.65458, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 777/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 115.4720 - acc: 0.2611 - val_loss: 92.4582 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00777: val_loss improved from 92.65458 to 92.45823, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 778/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 115.1431 - acc: 0.2611 - val_loss: 92.3322 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00778: val_loss improved from 92.45823 to 92.33224, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 779/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 114.8073 - acc: 0.2611 - val_loss: 92.1289 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00779: val_loss improved from 92.33224 to 92.12892, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 780/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 114.4732 - acc: 0.2611 - val_loss: 91.9134 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00780: val_loss improved from 92.12892 to 91.91338, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 781/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 114.1427 - acc: 0.2611 - val_loss: 91.7026 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00781: val_loss improved from 91.91338 to 91.70257, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 782/950\n",
            "360/360 [==============================] - 0s 209us/step - loss: 113.8220 - acc: 0.2611 - val_loss: 91.4658 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00782: val_loss improved from 91.70257 to 91.46584, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 783/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 113.5137 - acc: 0.2611 - val_loss: 91.3566 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00783: val_loss improved from 91.46584 to 91.35662, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 784/950\n",
            "360/360 [==============================] - 0s 219us/step - loss: 113.1619 - acc: 0.2611 - val_loss: 91.1361 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00784: val_loss improved from 91.35662 to 91.13612, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 785/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 112.8424 - acc: 0.2611 - val_loss: 90.9288 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00785: val_loss improved from 91.13612 to 90.92882, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 786/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 112.5252 - acc: 0.2611 - val_loss: 90.7125 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00786: val_loss improved from 90.92882 to 90.71246, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 787/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 112.2042 - acc: 0.2639 - val_loss: 90.5928 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00787: val_loss improved from 90.71246 to 90.59279, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 788/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 111.8807 - acc: 0.2611 - val_loss: 90.3024 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00788: val_loss improved from 90.59279 to 90.30244, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 789/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 111.5632 - acc: 0.2639 - val_loss: 90.1520 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00789: val_loss improved from 90.30244 to 90.15197, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 790/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 111.2423 - acc: 0.2639 - val_loss: 89.9032 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00790: val_loss improved from 90.15197 to 89.90316, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 791/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 110.9268 - acc: 0.2639 - val_loss: 89.7047 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00791: val_loss improved from 89.90316 to 89.70466, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 792/950\n",
            "360/360 [==============================] - 0s 260us/step - loss: 110.6230 - acc: 0.2639 - val_loss: 89.6120 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00792: val_loss improved from 89.70466 to 89.61201, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 793/950\n",
            "360/360 [==============================] - 0s 256us/step - loss: 110.2929 - acc: 0.2639 - val_loss: 89.4216 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00793: val_loss improved from 89.61201 to 89.42160, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 794/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 109.9685 - acc: 0.2639 - val_loss: 89.1911 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00794: val_loss improved from 89.42160 to 89.19106, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 795/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 109.6597 - acc: 0.2611 - val_loss: 88.9982 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00795: val_loss improved from 89.19106 to 88.99823, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 796/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 109.3486 - acc: 0.2639 - val_loss: 88.7671 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00796: val_loss improved from 88.99823 to 88.76707, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 797/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 109.0419 - acc: 0.2639 - val_loss: 88.6278 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00797: val_loss improved from 88.76707 to 88.62778, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 798/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 108.7228 - acc: 0.2639 - val_loss: 88.3492 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00798: val_loss improved from 88.62778 to 88.34920, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 799/950\n",
            "360/360 [==============================] - 0s 228us/step - loss: 108.4196 - acc: 0.2639 - val_loss: 88.0999 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00799: val_loss improved from 88.34920 to 88.09993, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 800/950\n",
            "360/360 [==============================] - 0s 294us/step - loss: 108.1105 - acc: 0.2639 - val_loss: 87.8967 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00800: val_loss improved from 88.09993 to 87.89668, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 801/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 107.8064 - acc: 0.2611 - val_loss: 87.8133 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00801: val_loss improved from 87.89668 to 87.81331, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 802/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 107.4932 - acc: 0.2639 - val_loss: 87.6163 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00802: val_loss improved from 87.81331 to 87.61633, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 803/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 107.1876 - acc: 0.2639 - val_loss: 87.3980 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00803: val_loss improved from 87.61633 to 87.39803, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 804/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 106.8920 - acc: 0.2639 - val_loss: 87.2256 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00804: val_loss improved from 87.39803 to 87.22559, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 805/950\n",
            "360/360 [==============================] - 0s 296us/step - loss: 106.5915 - acc: 0.2639 - val_loss: 87.0298 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00805: val_loss improved from 87.22559 to 87.02981, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 806/950\n",
            "360/360 [==============================] - 0s 305us/step - loss: 106.2719 - acc: 0.2639 - val_loss: 86.7625 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00806: val_loss improved from 87.02981 to 86.76248, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 807/950\n",
            "360/360 [==============================] - 0s 263us/step - loss: 105.9730 - acc: 0.2639 - val_loss: 86.5643 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00807: val_loss improved from 86.76248 to 86.56435, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 808/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 105.6767 - acc: 0.2639 - val_loss: 86.4028 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00808: val_loss improved from 86.56435 to 86.40285, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 809/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 105.3715 - acc: 0.2639 - val_loss: 86.1387 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00809: val_loss improved from 86.40285 to 86.13868, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 810/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 105.0730 - acc: 0.2639 - val_loss: 85.9621 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00810: val_loss improved from 86.13868 to 85.96212, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 811/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 104.7811 - acc: 0.2639 - val_loss: 85.7381 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00811: val_loss improved from 85.96212 to 85.73807, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 812/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 104.4797 - acc: 0.2639 - val_loss: 85.6059 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00812: val_loss improved from 85.73807 to 85.60587, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 813/950\n",
            "360/360 [==============================] - 0s 262us/step - loss: 104.1884 - acc: 0.2667 - val_loss: 85.4357 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00813: val_loss improved from 85.60587 to 85.43573, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 814/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 103.8914 - acc: 0.2667 - val_loss: 85.2475 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00814: val_loss improved from 85.43573 to 85.24749, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 815/950\n",
            "360/360 [==============================] - 0s 247us/step - loss: 103.6011 - acc: 0.2667 - val_loss: 84.9485 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00815: val_loss improved from 85.24749 to 84.94854, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 816/950\n",
            "360/360 [==============================] - 0s 266us/step - loss: 103.3037 - acc: 0.2667 - val_loss: 84.8249 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00816: val_loss improved from 84.94854 to 84.82491, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 817/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 103.0104 - acc: 0.2667 - val_loss: 84.6443 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00817: val_loss improved from 84.82491 to 84.64428, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 818/950\n",
            "360/360 [==============================] - 0s 260us/step - loss: 102.7234 - acc: 0.2667 - val_loss: 84.3979 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00818: val_loss improved from 84.64428 to 84.39792, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 819/950\n",
            "360/360 [==============================] - 0s 252us/step - loss: 102.4277 - acc: 0.2667 - val_loss: 84.1649 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00819: val_loss improved from 84.39792 to 84.16489, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 820/950\n",
            "360/360 [==============================] - 0s 259us/step - loss: 102.1394 - acc: 0.2667 - val_loss: 84.0209 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00820: val_loss improved from 84.16489 to 84.02093, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 821/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 101.8547 - acc: 0.2667 - val_loss: 83.8111 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00821: val_loss improved from 84.02093 to 83.81114, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 822/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 101.5717 - acc: 0.2667 - val_loss: 83.6042 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00822: val_loss improved from 83.81114 to 83.60423, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 823/950\n",
            "360/360 [==============================] - 0s 267us/step - loss: 101.2765 - acc: 0.2667 - val_loss: 83.4368 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00823: val_loss improved from 83.60423 to 83.43678, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 824/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 100.9875 - acc: 0.2667 - val_loss: 83.2283 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00824: val_loss improved from 83.43678 to 83.22834, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 825/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 100.7044 - acc: 0.2667 - val_loss: 83.0424 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00825: val_loss improved from 83.22834 to 83.04239, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 826/950\n",
            "360/360 [==============================] - 0s 240us/step - loss: 100.4201 - acc: 0.2667 - val_loss: 82.8771 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00826: val_loss improved from 83.04239 to 82.87710, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 827/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 100.1352 - acc: 0.2667 - val_loss: 82.6164 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00827: val_loss improved from 82.87710 to 82.61643, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 828/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 99.8552 - acc: 0.2667 - val_loss: 82.4195 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00828: val_loss improved from 82.61643 to 82.41952, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 829/950\n",
            "360/360 [==============================] - 0s 292us/step - loss: 99.5666 - acc: 0.2667 - val_loss: 82.2632 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00829: val_loss improved from 82.41952 to 82.26318, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 830/950\n",
            "360/360 [==============================] - 0s 241us/step - loss: 99.2888 - acc: 0.2667 - val_loss: 82.0352 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00830: val_loss improved from 82.26318 to 82.03522, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 831/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 99.0052 - acc: 0.2667 - val_loss: 81.8793 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00831: val_loss improved from 82.03522 to 81.87927, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 832/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 98.7290 - acc: 0.2667 - val_loss: 81.6267 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00832: val_loss improved from 81.87927 to 81.62674, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 833/950\n",
            "360/360 [==============================] - 0s 250us/step - loss: 98.4427 - acc: 0.2667 - val_loss: 81.4622 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00833: val_loss improved from 81.62674 to 81.46216, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 834/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 98.1707 - acc: 0.2667 - val_loss: 81.3189 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00834: val_loss improved from 81.46216 to 81.31886, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 835/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 97.8918 - acc: 0.2667 - val_loss: 81.0672 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00835: val_loss improved from 81.31886 to 81.06724, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 836/950\n",
            "360/360 [==============================] - 0s 282us/step - loss: 97.6115 - acc: 0.2667 - val_loss: 80.8822 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00836: val_loss improved from 81.06724 to 80.88219, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 837/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 97.3451 - acc: 0.2667 - val_loss: 80.7375 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00837: val_loss improved from 80.88219 to 80.73747, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 838/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 97.0651 - acc: 0.2667 - val_loss: 80.5500 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00838: val_loss improved from 80.73747 to 80.55001, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 839/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 96.7866 - acc: 0.2667 - val_loss: 80.3072 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00839: val_loss improved from 80.55001 to 80.30716, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 840/950\n",
            "360/360 [==============================] - 0s 258us/step - loss: 96.5343 - acc: 0.2667 - val_loss: 80.1704 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00840: val_loss improved from 80.30716 to 80.17040, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 841/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 96.2476 - acc: 0.2667 - val_loss: 79.9317 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00841: val_loss improved from 80.17040 to 79.93169, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 842/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 95.9810 - acc: 0.2667 - val_loss: 79.7264 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00842: val_loss improved from 79.93169 to 79.72645, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 843/950\n",
            "360/360 [==============================] - 0s 256us/step - loss: 95.7197 - acc: 0.2667 - val_loss: 79.5885 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00843: val_loss improved from 79.72645 to 79.58855, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 844/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 95.4361 - acc: 0.2667 - val_loss: 79.3711 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00844: val_loss improved from 79.58855 to 79.37110, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 845/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 95.1677 - acc: 0.2667 - val_loss: 79.1454 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00845: val_loss improved from 79.37110 to 79.14540, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 846/950\n",
            "360/360 [==============================] - 0s 238us/step - loss: 94.9054 - acc: 0.2667 - val_loss: 79.0239 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00846: val_loss improved from 79.14540 to 79.02394, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 847/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 94.6344 - acc: 0.2667 - val_loss: 78.7794 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00847: val_loss improved from 79.02394 to 78.77941, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 848/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 94.3715 - acc: 0.2667 - val_loss: 78.6189 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00848: val_loss improved from 78.77941 to 78.61887, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 849/950\n",
            "360/360 [==============================] - 0s 209us/step - loss: 94.0950 - acc: 0.2667 - val_loss: 78.3997 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00849: val_loss improved from 78.61887 to 78.39969, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 850/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 93.8394 - acc: 0.2667 - val_loss: 78.1962 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00850: val_loss improved from 78.39969 to 78.19620, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 851/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 93.5745 - acc: 0.2667 - val_loss: 77.9753 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00851: val_loss improved from 78.19620 to 77.97531, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 852/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 93.3051 - acc: 0.2667 - val_loss: 77.7913 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00852: val_loss improved from 77.97531 to 77.79135, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 853/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 93.0424 - acc: 0.2667 - val_loss: 77.6275 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00853: val_loss improved from 77.79135 to 77.62752, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 854/950\n",
            "360/360 [==============================] - 0s 253us/step - loss: 92.7804 - acc: 0.2667 - val_loss: 77.4549 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00854: val_loss improved from 77.62752 to 77.45485, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 855/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 92.5313 - acc: 0.2639 - val_loss: 77.3368 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00855: val_loss improved from 77.45485 to 77.33680, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 856/950\n",
            "360/360 [==============================] - 0s 243us/step - loss: 92.2538 - acc: 0.2639 - val_loss: 77.0649 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00856: val_loss improved from 77.33680 to 77.06493, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 857/950\n",
            "360/360 [==============================] - 0s 276us/step - loss: 92.0014 - acc: 0.2639 - val_loss: 76.9359 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00857: val_loss improved from 77.06493 to 76.93586, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 858/950\n",
            "360/360 [==============================] - 0s 268us/step - loss: 91.7372 - acc: 0.2639 - val_loss: 76.6802 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00858: val_loss improved from 76.93586 to 76.68025, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 859/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 91.4919 - acc: 0.2639 - val_loss: 76.5715 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00859: val_loss improved from 76.68025 to 76.57148, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 860/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 91.2306 - acc: 0.2639 - val_loss: 76.3754 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00860: val_loss improved from 76.57148 to 76.37543, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 861/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 90.9652 - acc: 0.2639 - val_loss: 76.1595 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00861: val_loss improved from 76.37543 to 76.15948, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 862/950\n",
            "360/360 [==============================] - 0s 258us/step - loss: 90.7136 - acc: 0.2639 - val_loss: 75.9471 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00862: val_loss improved from 76.15948 to 75.94709, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 863/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 90.4513 - acc: 0.2639 - val_loss: 75.7508 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00863: val_loss improved from 75.94709 to 75.75076, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 864/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 90.2042 - acc: 0.2639 - val_loss: 75.5822 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00864: val_loss improved from 75.75076 to 75.58218, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 865/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 89.9536 - acc: 0.2611 - val_loss: 75.4608 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00865: val_loss improved from 75.58218 to 75.46081, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 866/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 89.6916 - acc: 0.2639 - val_loss: 75.2602 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00866: val_loss improved from 75.46081 to 75.26024, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 867/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 89.4449 - acc: 0.2639 - val_loss: 75.0512 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00867: val_loss improved from 75.26024 to 75.05115, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 868/950\n",
            "360/360 [==============================] - 0s 270us/step - loss: 89.1927 - acc: 0.2611 - val_loss: 74.8315 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00868: val_loss improved from 75.05115 to 74.83151, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 869/950\n",
            "360/360 [==============================] - 0s 213us/step - loss: 88.9487 - acc: 0.2611 - val_loss: 74.6860 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00869: val_loss improved from 74.83151 to 74.68602, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 870/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 88.7017 - acc: 0.2611 - val_loss: 74.5016 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00870: val_loss improved from 74.68602 to 74.50158, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 871/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 88.4479 - acc: 0.2611 - val_loss: 74.2689 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00871: val_loss improved from 74.50158 to 74.26895, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 872/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 88.2149 - acc: 0.2583 - val_loss: 74.1036 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00872: val_loss improved from 74.26895 to 74.10361, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 873/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 87.9562 - acc: 0.2583 - val_loss: 73.9277 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00873: val_loss improved from 74.10361 to 73.92766, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 874/950\n",
            "360/360 [==============================] - 0s 247us/step - loss: 87.7157 - acc: 0.2583 - val_loss: 73.7962 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00874: val_loss improved from 73.92766 to 73.79618, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 875/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 87.4635 - acc: 0.2583 - val_loss: 73.6182 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00875: val_loss improved from 73.79618 to 73.61818, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 876/950\n",
            "360/360 [==============================] - 0s 215us/step - loss: 87.2208 - acc: 0.2611 - val_loss: 73.4027 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00876: val_loss improved from 73.61818 to 73.40274, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 877/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 86.9808 - acc: 0.2583 - val_loss: 73.1762 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00877: val_loss improved from 73.40274 to 73.17622, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 878/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 86.7390 - acc: 0.2611 - val_loss: 73.0718 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00878: val_loss improved from 73.17622 to 73.07177, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 879/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 86.4950 - acc: 0.2583 - val_loss: 72.8311 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00879: val_loss improved from 73.07177 to 72.83114, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 880/950\n",
            "360/360 [==============================] - 0s 270us/step - loss: 86.2594 - acc: 0.2583 - val_loss: 72.7237 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00880: val_loss improved from 72.83114 to 72.72374, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 881/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 86.0124 - acc: 0.2611 - val_loss: 72.5336 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00881: val_loss improved from 72.72374 to 72.53355, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 882/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 85.7789 - acc: 0.2611 - val_loss: 72.3730 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00882: val_loss improved from 72.53355 to 72.37302, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 883/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 85.5288 - acc: 0.2611 - val_loss: 72.1835 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00883: val_loss improved from 72.37302 to 72.18347, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 884/950\n",
            "360/360 [==============================] - 0s 251us/step - loss: 85.2991 - acc: 0.2611 - val_loss: 71.9591 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00884: val_loss improved from 72.18347 to 71.95914, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 885/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 85.0642 - acc: 0.2611 - val_loss: 71.8530 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00885: val_loss improved from 71.95914 to 71.85297, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 886/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 84.8367 - acc: 0.2611 - val_loss: 71.5956 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00886: val_loss improved from 71.85297 to 71.59561, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 887/950\n",
            "360/360 [==============================] - 0s 245us/step - loss: 84.5900 - acc: 0.2583 - val_loss: 71.4943 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00887: val_loss improved from 71.59561 to 71.49434, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 888/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 84.3508 - acc: 0.2583 - val_loss: 71.3046 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00888: val_loss improved from 71.49434 to 71.30459, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 889/950\n",
            "360/360 [==============================] - 0s 278us/step - loss: 84.1246 - acc: 0.2556 - val_loss: 71.1315 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00889: val_loss improved from 71.30459 to 71.13147, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 890/950\n",
            "360/360 [==============================] - 0s 217us/step - loss: 83.8813 - acc: 0.2556 - val_loss: 70.9747 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00890: val_loss improved from 71.13147 to 70.97470, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 891/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 83.6431 - acc: 0.2556 - val_loss: 70.7809 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00891: val_loss improved from 70.97470 to 70.78089, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 892/950\n",
            "360/360 [==============================] - 0s 221us/step - loss: 83.4130 - acc: 0.2556 - val_loss: 70.6347 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00892: val_loss improved from 70.78089 to 70.63467, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 893/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 83.1829 - acc: 0.2556 - val_loss: 70.4200 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00893: val_loss improved from 70.63467 to 70.42001, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 894/950\n",
            "360/360 [==============================] - 0s 289us/step - loss: 82.9567 - acc: 0.2556 - val_loss: 70.3084 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00894: val_loss improved from 70.42001 to 70.30838, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 895/950\n",
            "360/360 [==============================] - 0s 214us/step - loss: 82.7231 - acc: 0.2556 - val_loss: 70.1245 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00895: val_loss improved from 70.30838 to 70.12448, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 896/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 82.4901 - acc: 0.2556 - val_loss: 69.9182 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00896: val_loss improved from 70.12448 to 69.91817, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 897/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 82.2655 - acc: 0.2556 - val_loss: 69.7652 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00897: val_loss improved from 69.91817 to 69.76521, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 898/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 82.0318 - acc: 0.2556 - val_loss: 69.5641 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00898: val_loss improved from 69.76521 to 69.56411, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 899/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 81.8044 - acc: 0.2556 - val_loss: 69.4323 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00899: val_loss improved from 69.56411 to 69.43230, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 900/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 81.5792 - acc: 0.2556 - val_loss: 69.2541 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00900: val_loss improved from 69.43230 to 69.25409, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 901/950\n",
            "360/360 [==============================] - 0s 211us/step - loss: 81.3499 - acc: 0.2556 - val_loss: 69.0361 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00901: val_loss improved from 69.25409 to 69.03614, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 902/950\n",
            "360/360 [==============================] - 0s 246us/step - loss: 81.1313 - acc: 0.2556 - val_loss: 68.9524 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00902: val_loss improved from 69.03614 to 68.95244, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 903/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 80.9000 - acc: 0.2556 - val_loss: 68.7747 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00903: val_loss improved from 68.95244 to 68.77469, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 904/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 80.6792 - acc: 0.2556 - val_loss: 68.6261 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00904: val_loss improved from 68.77469 to 68.62612, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 905/950\n",
            "360/360 [==============================] - 0s 273us/step - loss: 80.4484 - acc: 0.2556 - val_loss: 68.4293 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00905: val_loss improved from 68.62612 to 68.42926, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 906/950\n",
            "360/360 [==============================] - 0s 208us/step - loss: 80.2310 - acc: 0.2556 - val_loss: 68.2559 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00906: val_loss improved from 68.42926 to 68.25586, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 907/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 80.0016 - acc: 0.2556 - val_loss: 68.1046 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00907: val_loss improved from 68.25586 to 68.10456, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 908/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 79.8032 - acc: 0.2556 - val_loss: 67.9140 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00908: val_loss improved from 68.10456 to 67.91397, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 909/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 79.5620 - acc: 0.2556 - val_loss: 67.7575 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00909: val_loss improved from 67.91397 to 67.75747, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 910/950\n",
            "360/360 [==============================] - 0s 226us/step - loss: 79.3413 - acc: 0.2556 - val_loss: 67.5547 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00910: val_loss improved from 67.75747 to 67.55470, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 911/950\n",
            "360/360 [==============================] - 0s 261us/step - loss: 79.1224 - acc: 0.2556 - val_loss: 67.4280 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00911: val_loss improved from 67.55470 to 67.42796, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 912/950\n",
            "360/360 [==============================] - 0s 212us/step - loss: 78.9014 - acc: 0.2556 - val_loss: 67.2100 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00912: val_loss improved from 67.42796 to 67.21002, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 913/950\n",
            "360/360 [==============================] - 0s 242us/step - loss: 78.6812 - acc: 0.2556 - val_loss: 67.1055 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00913: val_loss improved from 67.21002 to 67.10547, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 914/950\n",
            "360/360 [==============================] - 0s 224us/step - loss: 78.4611 - acc: 0.2556 - val_loss: 66.9412 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00914: val_loss improved from 67.10547 to 66.94121, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 915/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 78.2468 - acc: 0.2556 - val_loss: 66.7481 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00915: val_loss improved from 66.94121 to 66.74815, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 916/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 78.0302 - acc: 0.2556 - val_loss: 66.6327 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00916: val_loss improved from 66.74815 to 66.63273, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 917/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 77.8131 - acc: 0.2556 - val_loss: 66.4644 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00917: val_loss improved from 66.63273 to 66.46444, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 918/950\n",
            "360/360 [==============================] - 0s 231us/step - loss: 77.5929 - acc: 0.2556 - val_loss: 66.3075 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00918: val_loss improved from 66.46444 to 66.30750, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 919/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 77.3788 - acc: 0.2556 - val_loss: 66.1121 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00919: val_loss improved from 66.30750 to 66.11212, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 920/950\n",
            "360/360 [==============================] - 0s 233us/step - loss: 77.1617 - acc: 0.2556 - val_loss: 65.9449 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00920: val_loss improved from 66.11212 to 65.94486, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 921/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 76.9553 - acc: 0.2556 - val_loss: 65.7444 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00921: val_loss improved from 65.94486 to 65.74439, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 922/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 76.7417 - acc: 0.2556 - val_loss: 65.6933 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00922: val_loss improved from 65.74439 to 65.69332, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 923/950\n",
            "360/360 [==============================] - 0s 234us/step - loss: 76.5276 - acc: 0.2556 - val_loss: 65.4614 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00923: val_loss improved from 65.69332 to 65.46144, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 924/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 76.3156 - acc: 0.2556 - val_loss: 65.3302 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00924: val_loss improved from 65.46144 to 65.33020, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 925/950\n",
            "360/360 [==============================] - 0s 235us/step - loss: 76.1048 - acc: 0.2556 - val_loss: 65.2089 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00925: val_loss improved from 65.33020 to 65.20891, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 926/950\n",
            "360/360 [==============================] - 0s 237us/step - loss: 75.8924 - acc: 0.2556 - val_loss: 64.9807 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00926: val_loss improved from 65.20891 to 64.98069, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 927/950\n",
            "360/360 [==============================] - 0s 225us/step - loss: 75.6844 - acc: 0.2556 - val_loss: 64.8436 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00927: val_loss improved from 64.98069 to 64.84357, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 928/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 75.4752 - acc: 0.2556 - val_loss: 64.6830 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00928: val_loss improved from 64.84357 to 64.68303, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 929/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 75.2631 - acc: 0.2556 - val_loss: 64.5645 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00929: val_loss improved from 64.68303 to 64.56450, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 930/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 75.0559 - acc: 0.2556 - val_loss: 64.3974 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00930: val_loss improved from 64.56450 to 64.39737, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 931/950\n",
            "360/360 [==============================] - 0s 216us/step - loss: 74.8450 - acc: 0.2556 - val_loss: 64.2458 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00931: val_loss improved from 64.39737 to 64.24581, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 932/950\n",
            "360/360 [==============================] - 0s 249us/step - loss: 74.6385 - acc: 0.2556 - val_loss: 64.1065 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00932: val_loss improved from 64.24581 to 64.10649, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 933/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 74.4371 - acc: 0.2556 - val_loss: 63.9501 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00933: val_loss improved from 64.10649 to 63.95010, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 934/950\n",
            "360/360 [==============================] - 0s 254us/step - loss: 74.2294 - acc: 0.2556 - val_loss: 63.7420 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00934: val_loss improved from 63.95010 to 63.74196, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 935/950\n",
            "360/360 [==============================] - 0s 229us/step - loss: 74.0318 - acc: 0.2556 - val_loss: 63.6315 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00935: val_loss improved from 63.74196 to 63.63146, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 936/950\n",
            "360/360 [==============================] - 0s 248us/step - loss: 73.8290 - acc: 0.2556 - val_loss: 63.4510 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00936: val_loss improved from 63.63146 to 63.45096, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 937/950\n",
            "360/360 [==============================] - 0s 244us/step - loss: 73.6203 - acc: 0.2556 - val_loss: 63.3124 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00937: val_loss improved from 63.45096 to 63.31238, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 938/950\n",
            "360/360 [==============================] - 0s 227us/step - loss: 73.4099 - acc: 0.2556 - val_loss: 63.1670 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00938: val_loss improved from 63.31238 to 63.16702, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 939/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 73.2161 - acc: 0.2556 - val_loss: 63.0093 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00939: val_loss improved from 63.16702 to 63.00930, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 940/950\n",
            "360/360 [==============================] - 0s 220us/step - loss: 73.0136 - acc: 0.2556 - val_loss: 62.8841 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00940: val_loss improved from 63.00930 to 62.88415, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 941/950\n",
            "360/360 [==============================] - 0s 230us/step - loss: 72.8085 - acc: 0.2556 - val_loss: 62.7072 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00941: val_loss improved from 62.88415 to 62.70717, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 942/950\n",
            "360/360 [==============================] - 0s 236us/step - loss: 72.6131 - acc: 0.2528 - val_loss: 62.5485 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00942: val_loss improved from 62.70717 to 62.54850, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 943/950\n",
            "360/360 [==============================] - 0s 264us/step - loss: 72.4166 - acc: 0.2556 - val_loss: 62.3999 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00943: val_loss improved from 62.54850 to 62.39988, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 944/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 72.2166 - acc: 0.2583 - val_loss: 62.2253 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00944: val_loss improved from 62.39988 to 62.22534, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 945/950\n",
            "360/360 [==============================] - 0s 222us/step - loss: 72.0172 - acc: 0.2583 - val_loss: 62.0904 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00945: val_loss improved from 62.22534 to 62.09038, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 946/950\n",
            "360/360 [==============================] - 0s 232us/step - loss: 71.8195 - acc: 0.2556 - val_loss: 61.9507 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00946: val_loss improved from 62.09038 to 61.95068, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 947/950\n",
            "360/360 [==============================] - 0s 317us/step - loss: 71.6239 - acc: 0.2583 - val_loss: 61.8273 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00947: val_loss improved from 61.95068 to 61.82732, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 948/950\n",
            "360/360 [==============================] - 0s 255us/step - loss: 71.4250 - acc: 0.2528 - val_loss: 61.6475 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00948: val_loss improved from 61.82732 to 61.64745, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 949/950\n",
            "360/360 [==============================] - 0s 223us/step - loss: 71.2308 - acc: 0.2583 - val_loss: 61.4952 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00949: val_loss improved from 61.64745 to 61.49517, saving model to ./log/benchmark_1.best.hdf5\n",
            "Epoch 950/950\n",
            "360/360 [==============================] - 0s 239us/step - loss: 71.0354 - acc: 0.2583 - val_loss: 61.3676 - val_acc: 0.1250\n",
            "\n",
            "Epoch 00950: val_loss improved from 61.49517 to 61.36758, saving model to ./log/benchmark_1.best.hdf5\n",
            "predict_seven_days ==  [ 75.1  75.2 106.3 106.4 106.7 106.9 107. ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA84AAAIECAYAAADIEXc+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACggOAMAAEABwRkAAAAKCM4AAABQ4H8BrDqo6wnBmTAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1200x640 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}